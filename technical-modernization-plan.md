# üõ†Ô∏è –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –ü–ª–∞–Ω –ú–æ–¥–µ—Ä–Ω–∏–∑–∞—Ü–∏–∏

## üìã –î–µ—Ç–∞–ª—å–Ω—ã–µ –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ó–∞–¥–∞—á–∏

### **–§–∞–∑–∞ 1: –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è**

#### **1.1 WebRTC –†–µ–∞–ª–∏–∑–∞—Ü–∏—è**

##### **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞**
```
WebRTC Module Structure:
‚îú‚îÄ‚îÄ webrtc/
‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ RTCPeerConnection.ts      # –û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ SignalingService.ts       # WebSocket —Å–∏–≥–Ω–∞–ª–∏–∑–∞—Ü–∏—è
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ MediaHandler.ts           # –ö–∞–º–µ—Ä–∞/–º–∏–∫—Ä–æ—Ñ–æ–Ω
‚îÇ   ‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ VideoCall.tsx             # UI –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –∑–≤–æ–Ω–∫–∞
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ CallControls.tsx          # –ö–Ω–æ–ø–∫–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ParticipantList.tsx       # –°–ø–∏—Å–æ–∫ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤
‚îÇ   ‚îú‚îÄ‚îÄ hooks/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ useWebRTC.ts              # –û—Å–Ω–æ–≤–Ω–æ–π hook
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ useMediaDevices.ts        # –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞–º–∏
‚îÇ   ‚îî‚îÄ‚îÄ types/
‚îÇ       ‚îî‚îÄ‚îÄ webrtc.types.ts           # TypeScript —Ç–∏–ø—ã
```

##### **–ö–ª—é—á–µ–≤—ã–µ –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã**

**RTCPeerConnection Manager:**
```typescript
class WebRTCManager {
  private peerConnections: Map<string, RTCPeerConnection> = new Map();
  private signalingService: SignalingService;

  async createPeerConnection(peerId: string): Promise<RTCPeerConnection> {
    const pc = new RTCPeerConnection({
      iceServers: [
        { urls: 'stun:stun.l.google.com:19302' },
        { urls: 'stun:stun1.l.google.com:19302' }
      ]
    });

    // Event handlers
    pc.onicecandidate = (event) => {
      if (event.candidate) {
        this.signalingService.send({
          type: 'ice-candidate',
          candidate: event.candidate,
          target: peerId
        });
      }
    };

    pc.ontrack = (event) => {
      // Handle remote stream
      this.handleRemoteStream(peerId, event.streams[0]);
    };

    this.peerConnections.set(peerId, pc);
    return pc;
  }

  async startCall(peerId: string): Promise<void> {
    const pc = await this.createPeerConnection(peerId);

    // Get local media
    const localStream = await navigator.mediaDevices.getUserMedia({
      audio: true,
      video: true
    });

    // Add tracks to peer connection
    localStream.getTracks().forEach(track => {
      pc.addTrack(track, localStream);
    });

    // Create offer
    const offer = await pc.createOffer();
    await pc.setLocalDescription(offer);

    // Send offer via signaling
    this.signalingService.send({
      type: 'offer',
      offer,
      target: peerId
    });
  }
}
```

**Signaling Service:**
```typescript
class SignalingService {
  private ws: WebSocket;
  private messageHandlers: Map<string, Function> = new Map();

  constructor(serverUrl: string) {
    this.ws = new WebSocket(serverUrl);

    this.ws.onmessage = (event) => {
      const message = JSON.parse(event.data);
      const handler = this.messageHandlers.get(message.type);
      if (handler) {
        handler(message);
      }
    };
  }

  on(type: string, handler: Function): void {
    this.messageHandlers.set(type, handler);
  }

  send(message: any): void {
    this.ws.send(JSON.stringify(message));
  }
}
```

##### **–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –°—É—â–µ—Å—Ç–≤—É—é—â–∏–º –ö–æ–¥–æ–º**
```typescript
// src/pages/VoiceChat.tsx - –¥–æ–±–∞–≤–∏—Ç—å –∏–º–ø–æ—Ä—Ç
import { WebRTCManager } from '@/webrtc/core/WebRTCManager';
import VideoCall from '@/webrtc/components/VideoCall';

// –î–æ–±–∞–≤–∏—Ç—å state
const [isInCall, setIsInCall] = useState(false);
const [callParticipants, setCallParticipants] = useState<string[]>([]);
const webRTCRef = useRef<WebRTCManager | null>(null);

// –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
useEffect(() => {
  webRTCRef.current = new WebRTCManager();
  webRTCRef.current.init();
}, []);

// –§—É–Ω–∫—Ü–∏—è –Ω–∞—á–∞–ª–∞ –∑–≤–æ–Ω–∫–∞
const startCall = async (targetUserId: string) => {
  try {
    await webRTCRef.current?.startCall(targetUserId);
    setIsInCall(true);
    setCallParticipants([targetUserId]);
  } catch (error) {
    toast.error('–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞—á–∞—Ç—å –∑–≤–æ–Ω–æ–∫: ' + error.message);
  }
};
```

#### **1.2 State Management Refactor**

##### **–¢–µ–∫—É—â–∏–µ –ü—Ä–æ–±–ª–µ–º—ã**
- 1218 —Å—Ç—Ä–æ–∫ –≤ VoiceChat.tsx
- –°–ª–æ–∂–Ω–∞—è –ª–æ–≥–∏–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏–π
- –ì–æ–Ω–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏–π
- –£—Ç–µ—á–∫–∏ –ø–∞–º—è—Ç–∏

##### **–†–µ—à–µ–Ω–∏–µ: State Machine**

**VoiceChatStateMachine.ts:**
```typescript
type VoiceChatState =
  | 'idle'
  | 'initializing'
  | 'listening'
  | 'processing'
  | 'speaking'
  | 'error'
  | 'call_active';

interface VoiceChatContext {
  isRecording: boolean;
  isTranscribing: boolean;
  isGeneratingResponse: boolean;
  isSpeaking: boolean;
  transcript: string;
  error?: string;
  callParticipants?: string[];
}

class VoiceChatStateMachine {
  private state: VoiceChatState = 'idle';
  private context: VoiceChatContext = {
    isRecording: false,
    isTranscribing: false,
    isGeneratingResponse: false,
    isSpeaking: false,
    transcript: ''
  };

  private listeners: Set<(state: VoiceChatState, context: VoiceChatContext) => void> = new Set();

  transition(newState: VoiceChatState, updates: Partial<VoiceChatContext> = {}): void {
    // Validate transition
    if (!this.isValidTransition(this.state, newState)) {
      console.warn(`Invalid transition: ${this.state} -> ${newState}`);
      return;
    }

    // Update context
    this.context = { ...this.context, ...updates };
    this.state = newState;

    // Cleanup based on state
    this.performCleanup();

    // Notify listeners
    this.listeners.forEach(listener => listener(this.state, this.context));
  }

  private isValidTransition(from: VoiceChatState, to: VoiceChatState): boolean {
    const validTransitions: Record<VoiceChatState, VoiceChatState[]> = {
      idle: ['initializing', 'call_active'],
      initializing: ['listening', 'error'],
      listening: ['processing', 'idle', 'error'],
      processing: ['speaking', 'idle', 'error'],
      speaking: ['listening', 'idle', 'error'],
      error: ['idle'],
      call_active: ['idle', 'error']
    };

    return validTransitions[from]?.includes(to) ?? false;
  }

  private performCleanup(): void {
    switch (this.state) {
      case 'idle':
        // Stop all activities
        this.context.isRecording = false;
        this.context.isTranscribing = false;
        this.context.isGeneratingResponse = false;
        this.context.isSpeaking = false;
        break;
      case 'error':
        // Stop everything on error
        this.context.isRecording = false;
        this.context.isTranscribing = false;
        this.context.isGeneratingResponse = false;
        this.context.isSpeaking = false;
        break;
    }
  }

  onStateChange(listener: (state: VoiceChatState, context: VoiceChatContext) => void): () => void {
    this.listeners.add(listener);
    return () => this.listeners.delete(listener);
  }

  getState(): VoiceChatState {
    return this.state;
  }

  getContext(): VoiceChatContext {
    return { ...this.context };
  }
}
```

##### **–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤ VoiceChat.tsx**
```typescript
// –ó–∞–º–µ–Ω–∏—Ç—å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ useState –Ω–∞ state machine
const stateMachineRef = useRef(new VoiceChatStateMachine());
const [currentState, setCurrentState] = useState<VoiceChatState>('idle');
const [context, setContext] = useState<VoiceChatContext>(stateMachineRef.current.getContext());

// –ü–æ–¥–ø–∏—Å–∫–∞ –Ω–∞ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è
useEffect(() => {
  const unsubscribe = stateMachineRef.current.onStateChange((state, newContext) => {
    setCurrentState(state);
    setContext(newContext);
  });

  return unsubscribe;
}, []);

// –£–ø—Ä–æ—â–µ–Ω–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è
const startListening = () => {
  stateMachineRef.current.transition('listening', { isRecording: true });
};

const stopListening = () => {
  stateMachineRef.current.transition('idle');
};

const handleError = (error: string) => {
  stateMachineRef.current.transition('error', { error });
};
```

#### **1.3 Error Handling & Resilience**

##### **Circuit Breaker Pattern**
```typescript
class CircuitBreaker {
  private failures = 0;
  private lastFailureTime = 0;
  private state: 'closed' | 'open' | 'half-open' = 'closed';

  private readonly failureThreshold = 5;
  private readonly recoveryTimeout = 60000; // 1 minute

  async execute<T>(operation: () => Promise<T>): Promise<T> {
    if (this.state === 'open') {
      if (Date.now() - this.lastFailureTime > this.recoveryTimeout) {
        this.state = 'half-open';
      } else {
        throw new Error('Circuit breaker is OPEN');
      }
    }

    try {
      const result = await operation();
      this.onSuccess();
      return result;
    } catch (error) {
      this.onFailure();
      throw error;
    }
  }

  private onSuccess(): void {
    this.failures = 0;
    this.state = 'closed';
  }

  private onFailure(): void {
    this.failures++;
    this.lastFailureTime = Date.now();

    if (this.failures >= this.failureThreshold) {
      this.state = 'open';
    }
  }

  getState(): string {
    return this.state;
  }
}
```

##### **Retry Logic —Å Exponential Backoff**
```typescript
class RetryManager {
  async executeWithRetry<T>(
    operation: () => Promise<T>,
    maxRetries: number = 3,
    baseDelay: number = 1000
  ): Promise<T> {
    let lastError: Error;

    for (let attempt = 0; attempt <= maxRetries; attempt++) {
      try {
        return await operation();
      } catch (error) {
        lastError = error as Error;

        if (attempt === maxRetries) {
          break;
        }

        // Exponential backoff with jitter
        const delay = baseDelay * Math.pow(2, attempt) + Math.random() * 1000;
        await new Promise(resolve => setTimeout(resolve, delay));
      }
    }

    throw lastError!;
  }
}
```

##### **Graceful Degradation**
```typescript
class FeatureManager {
  private features = new Map<string, boolean>();

  constructor() {
    this.detectFeatures();
  }

  private detectFeatures(): void {
    // WebRTC support
    this.features.set('webrtc', !!(window.RTCPeerConnection || window.webkitRTCPeerConnection));

    // Speech Recognition
    this.features.set('speechRecognition', !!(
      window.SpeechRecognition ||
      window.webkitSpeechRecognition ||
      window.mozSpeechRecognition
    ));

    // Web Audio
    this.features.set('webAudio', !!(window.AudioContext || window.webkitAudioContext));

    // Hardware concurrency
    this.features.set('highPerformance', navigator.hardwareConcurrency >= 4);

    // Memory (rough estimate)
    this.features.set('sufficientMemory', navigator.deviceMemory >= 4);
  }

  isEnabled(feature: string): boolean {
    return this.features.get(feature) ?? false;
  }

  getDegradationLevel(): 'full' | 'basic' | 'minimal' {
    const features = Array.from(this.features.values());
    const enabledCount = features.filter(Boolean).length;

    if (enabledCount >= 4) return 'full';
    if (enabledCount >= 2) return 'basic';
    return 'minimal';
  }
}
```

### **–§–∞–∑–∞ 2: –ë—Ä–∞—É–∑–µ—Ä–Ω–∞—è –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å**

#### **2.1 Universal Speech Recognition Polyfill**
```typescript
class UniversalSpeechRecognition {
  private recognition: SpeechRecognition | null = null;
  private isWebkit = false;
  private isMozilla = false;

  constructor() {
    this.detectBrowser();
    this.initialize();
  }

  private detectBrowser(): void {
    const ua = navigator.userAgent.toLowerCase();
    this.isWebkit = ua.includes('webkit') && !ua.includes('edge');
    this.isMozilla = ua.includes('firefox');
  }

  private initialize(): void {
    const SpeechRecognitionClass =
      window.SpeechRecognition ||
      window.webkitSpeechRecognition ||
      window.mozSpeechRecognition;

    if (!SpeechRecognitionClass) {
      throw new Error('Speech Recognition not supported');
    }

    this.recognition = new SpeechRecognitionClass();

    // Browser-specific optimizations
    if (this.isWebkit) {
      // Safari/WebKit specific settings
      this.recognition.continuous = false; // Safari has issues with continuous
      this.recognition.interimResults = false;
    } else if (this.isMozilla) {
      // Firefox specific settings
      this.recognition.continuous = true;
      this.recognition.interimResults = true;
    } else {
      // Chrome/Edge
      this.recognition.continuous = true;
      this.recognition.interimResults = true;
    }

    this.recognition.lang = 'ru-RU';
    this.recognition.maxAlternatives = 1;
  }

  start(): Promise<void> {
    return new Promise((resolve, reject) => {
      if (!this.recognition) {
        reject(new Error('Speech Recognition not initialized'));
        return;
      }

      const timeout = setTimeout(() => {
        reject(new Error('Speech Recognition timeout'));
      }, 10000);

      this.recognition.onstart = () => {
        clearTimeout(timeout);
        resolve();
      };

      this.recognition.onerror = (event) => {
        clearTimeout(timeout);
        reject(new Error(`Speech Recognition error: ${event.error}`));
      };

      this.recognition.start();
    });
  }

  stop(): void {
    this.recognition?.stop();
  }

  abort(): void {
    this.recognition?.abort();
  }

  onResult(handler: (event: SpeechRecognitionEvent) => void): void {
    if (this.recognition) {
      this.recognition.onresult = handler;
    }
  }

  onError(handler: (event: SpeechRecognitionErrorEvent) => void): void {
    if (this.recognition) {
      this.recognition.onerror = handler;
    }
  }

  onEnd(handler: () => void): void {
    if (this.recognition) {
      this.recognition.onend = handler;
    }
  }
}
```

#### **2.2 Enhanced Echo Detection System**
```typescript
class EnhancedEchoDetector {
  private audioContext: AudioContext;
  private analyser: AnalyserNode;
  private frequencyData: Uint8Array;
  private timeData: Uint8Array;
  private ttsProfile: TTSProfile | null = null;

  constructor(audioContext: AudioContext) {
    this.audioContext = audioContext;
    this.analyser = audioContext.createAnalyser();
    this.analyser.fftSize = 2048;
    this.analyser.smoothingTimeConstant = 0.8;

    const bufferLength = this.analyser.frequencyBinCount;
    this.frequencyData = new Uint8Array(bufferLength);
    this.timeData = new Uint8Array(bufferLength);
  }

  analyzeAudio(): AudioFeatures {
    this.analyser.getByteFrequencyData(this.frequencyData);
    this.analyser.getByteTimeDomainData(this.timeData);

    return {
      rms: this.calculateRMS(),
      spectralCentroid: this.calculateSpectralCentroid(),
      dominantFrequencies: this.findDominantFrequencies(),
      zeroCrossings: this.calculateZeroCrossings()
    };
  }

  detectEcho(ttsText: string, userInput: string, audioFeatures: AudioFeatures): EchoResult {
    const textSimilarity = this.calculateTextSimilarity(ttsText, userInput);
    const frequencySimilarity = this.ttsProfile ?
      this.calculateFrequencySimilarity(audioFeatures, this.ttsProfile) : 0;

    const combinedScore = (
      textSimilarity * 0.6 +
      frequencySimilarity * 0.4
    );

    return {
      isEcho: combinedScore > 0.7,
      confidence: combinedScore,
      methods: {
        text: textSimilarity > 0.6,
        frequency: frequencySimilarity > 0.8
      }
    };
  }

  private calculateTextSimilarity(text1: string, text2: string): number {
    // Levenshtein distance based similarity
    const distance = this.levenshteinDistance(text1.toLowerCase(), text2.toLowerCase());
    const maxLength = Math.max(text1.length, text2.length);
    return maxLength === 0 ? 1 : (maxLength - distance) / maxLength;
  }

  private calculateFrequencySimilarity(features: AudioFeatures, profile: TTSProfile): number {
    // Compare dominant frequencies
    let similarity = 0;
    const profileFreqs = profile.dominantFrequencies;

    features.dominantFrequencies.forEach(featureFreq => {
      const closest = profileFreqs.reduce((prev, curr) =>
        Math.abs(curr.frequency - featureFreq.frequency) < Math.abs(prev.frequency - featureFreq.frequency)
          ? curr : prev
      );

      const freqDiff = Math.abs(closest.frequency - featureFreq.frequency);
      const ampDiff = Math.abs(closest.amplitude - featureFreq.amplitude);

      if (freqDiff < 300) { // 300Hz tolerance
        similarity += (1 - freqDiff / 300) * (1 - Math.min(ampDiff, 1));
      }
    });

    return similarity / features.dominantFrequencies.length;
  }

  private calculateRMS(): number {
    let sum = 0;
    for (let i = 0; i < this.timeData.length; i++) {
      const sample = (this.timeData[i] - 128) / 128; // Convert to -1 to 1
      sum += sample * sample;
    }
    return Math.sqrt(sum / this.timeData.length);
  }

  private calculateSpectralCentroid(): number {
    let numerator = 0;
    let denominator = 0;

    for (let i = 0; i < this.frequencyData.length; i++) {
      const magnitude = this.frequencyData[i];
      const frequency = (i * this.audioContext.sampleRate) / (2 * this.frequencyData.length);

      numerator += frequency * magnitude;
      denominator += magnitude;
    }

    return denominator === 0 ? 0 : numerator / denominator;
  }

  private findDominantFrequencies(): Array<{frequency: number, amplitude: number}> {
    const peaks: Array<{frequency: number, amplitude: number}> = [];
    const minAmplitude = 180; // Threshold

    for (let i = 1; i < this.frequencyData.length - 1; i++) {
      const amplitude = this.frequencyData[i];
      const prevAmplitude = this.frequencyData[i - 1];
      const nextAmplitude = this.frequencyData[i + 1];

      if (amplitude > minAmplitude && amplitude > prevAmplitude && amplitude > nextAmplitude) {
        const frequency = (i * this.audioContext.sampleRate) / (2 * this.frequencyData.length);
        peaks.push({ frequency, amplitude });
      }
    }

    return peaks.slice(0, 5); // Top 5 peaks
  }

  private calculateZeroCrossings(): number {
    let crossings = 0;
    for (let i = 1; i < this.timeData.length; i++) {
      const current = this.timeData[i] - 128;
      const previous = this.timeData[i - 1] - 128;

      if ((current > 0 && previous <= 0) || (current < 0 && previous >= 0)) {
        crossings++;
      }
    }
    return crossings;
  }

  private levenshteinDistance(str1: string, str2: string): number {
    const matrix = [];
    for (let i = 0; i <= str2.length; i++) {
      matrix[i] = [i];
    }
    for (let j = 0; j <= str1.length; j++) {
      matrix[0][j] = j;
    }
    for (let i = 1; i <= str2.length; i++) {
      for (let j = 1; j <= str1.length; j++) {
        if (str2.charAt(i - 1) === str1.charAt(j - 1)) {
          matrix[i][j] = matrix[i - 1][j - 1];
        } else {
          matrix[i][j] = Math.min(
            matrix[i - 1][j - 1] + 1,
            matrix[i][j - 1] + 1,
            matrix[i - 1][j] + 1
          );
        }
      }
    }
    return matrix[str2.length][str1.length];
  }
}
```

### **–§–∞–∑–∞ 3: –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥**

#### **3.1 Performance Monitoring**
```typescript
class PerformanceMonitor {
  private metrics: Map<string, number[]> = new Map();
  private alerts: Map<string, (value: number) => void> = new Map();

  measure<T>(name: string, operation: () => T): T {
    const start = performance.now();
    try {
      const result = operation();
      const duration = performance.now() - start;
      this.recordMetric(name, duration);
      return result;
    } catch (error) {
      const duration = performance.now() - start;
      this.recordMetric(`${name}_error`, duration);
      throw error;
    }
  }

  async measureAsync<T>(name: string, operation: () => Promise<T>): Promise<T> {
    const start = performance.now();
    try {
      const result = await operation();
      const duration = performance.now() - start;
      this.recordMetric(name, duration);
      return result;
    } catch (error) {
      const duration = performance.now() - start;
      this.recordMetric(`${name}_error`, duration);
      throw error;
    }
  }

  private recordMetric(name: string, value: number): void {
    if (!this.metrics.has(name)) {
      this.metrics.set(name, []);
    }

    const values = this.metrics.get(name)!;
    values.push(value);

    // Keep only last 100 measurements
    if (values.length > 100) {
      values.shift();
    }

    // Check alerts
    const alertFn = this.alerts.get(name);
    if (alertFn) {
      alertFn(value);
    }
  }

  getAverage(name: string): number {
    const values = this.metrics.get(name) || [];
    return values.length === 0 ? 0 : values.reduce((a, b) => a + b, 0) / values.length;
  }

  getPercentile(name: string, percentile: number): number {
    const values = [...(this.metrics.get(name) || [])].sort((a, b) => a - b);
    if (values.length === 0) return 0;

    const index = Math.floor((percentile / 100) * values.length);
    return values[Math.min(index, values.length - 1)];
  }

  setAlert(name: string, threshold: number, callback: (value: number) => void): void {
    this.alerts.set(name, (value) => {
      if (value > threshold) {
        callback(value);
      }
    });
  }

  getReport(): Record<string, any> {
    const report: Record<string, any> = {};

    for (const [name, values] of this.metrics) {
      report[name] = {
        count: values.length,
        average: this.getAverage(name),
        p50: this.getPercentile(name, 50),
        p95: this.getPercentile(name, 95),
        p99: this.getPercentile(name, 99),
        min: Math.min(...values),
        max: Math.max(...values)
      };
    }

    return report;
  }
}
```

#### **3.2 Memory Management**
```typescript
class MemoryManager {
  private cleanupTasks: Set<() => void> = new Set();
  private audioContexts: Set<AudioContext> = new Set();
  private mediaStreams: Set<MediaStream> = new Set();

  registerAudioContext(context: AudioContext): () => void {
    this.audioContexts.add(context);

    const cleanup = () => {
      this.audioContexts.delete(context);
      if (context.state !== 'closed') {
        context.close().catch(console.warn);
      }
    };

    this.cleanupTasks.add(cleanup);
    return cleanup;
  }

  registerMediaStream(stream: MediaStream): () => void {
    this.mediaStreams.add(stream);

    const cleanup = () => {
      this.mediaStreams.delete(stream);
      stream.getTracks().forEach(track => {
        track.stop();
      });
    };

    this.cleanupTasks.add(cleanup);
    return cleanup;
  }

  cleanup(): void {
    for (const cleanup of this.cleanupTasks) {
      try {
        cleanup();
      } catch (error) {
        console.warn('Cleanup error:', error);
      }
    }
    this.cleanupTasks.clear();
  }

  getMemoryUsage(): MemoryInfo {
    if ('memory' in performance) {
      const mem = (performance as any).memory;
      return {
        used: mem.usedJSHeapSize,
        total: mem.totalJSHeapSize,
        limit: mem.jsHeapSizeLimit,
        usagePercent: (mem.usedJSHeapSize / mem.jsHeapSizeLimit) * 100
      };
    }

    // Fallback for browsers without memory API
    return {
      used: 0,
      total: 0,
      limit: 0,
      usagePercent: 0
    };
  }

  scheduleCleanup(interval: number = 30000): void {
    setInterval(() => {
      const memoryUsage = this.getMemoryUsage();

      // Force cleanup if memory usage is high
      if (memoryUsage.usagePercent > 80) {
        console.warn('High memory usage detected, performing cleanup');
        this.cleanup();
      }
    }, interval);
  }
}

interface MemoryInfo {
  used: number;
  total: number;
  limit: number;
  usagePercent: number;
}
```

## üìã –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ –§–∞–∑–∞–º

### **–§–∞–∑–∞ 1: –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è (1-2 –Ω–µ–¥–µ–ª–∏)**
1. **–î–µ–Ω—å 1-3**: WebRTC –±–∞–∑–æ–≤–∞—è –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞
2. **–î–µ–Ω—å 4-5**: State machine —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥
3. **–î–µ–Ω—å 6-7**: Error handling –∏ resilience
4. **–î–µ–Ω—å 8-10**: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è

### **–§–∞–∑–∞ 2: –°—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è (2-4 –Ω–µ–¥–µ–ª–∏)**
1. **–ù–µ–¥–µ–ª—è 1**: –ë—Ä–∞—É–∑–µ—Ä–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å
2. **–ù–µ–¥–µ–ª—è 2**: Echo detection v2.0
3. **–ù–µ–¥–µ–ª—è 3**: –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
4. **–ù–µ–¥–µ–ª—è 4**: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

### **–§–∞–∑–∞ 3: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è UX (2-3 –Ω–µ–¥–µ–ª–∏)**
1. **–ù–µ–¥–µ–ª—è 1**: UX/UI —É–ª—É—á—à–µ–Ω–∏—è
2. **–ù–µ–¥–µ–ª—è 2**: Offline —Ä–µ–∂–∏–º –∏ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ—Å—Ç—å
3. **–ù–µ–¥–µ–ª—è 3**: –ù–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∏ —Ñ–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è

### **–§–∞–∑–∞ 4: –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ (3-4 –Ω–µ–¥–µ–ª–∏)**
1. **–ù–µ–¥–µ–ª—è 1**: –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è
2. **–ù–µ–¥–µ–ª—è 2**: –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞
3. **–ù–µ–¥–µ–ª—è 3**: –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
4. **–ù–µ–¥–µ–ª—è 4**: Production readiness

## üéØ –ö–ª—é—á–µ–≤—ã–µ –†–∏—Å–∫–∏ –∏ –†–µ—à–µ–Ω–∏—è

### **–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –†–∏—Å–∫–∏**
- **WebRTC complexity**: –ù–∞—á–∞—Ç—å —Å –ø—Ä–æ—Å—Ç–æ–≥–æ P2P, –∑–∞—Ç–µ–º –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å
- **State management**: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å proven –ø–∞—Ç—Ç–µ—Ä–Ω—ã (state machine)
- **Browser compatibility**: Progressive enhancement —Å fallbacks

### **–ü—Ä–æ–µ–∫—Ç–Ω—ã–µ –†–∏—Å–∫–∏**
- **Scope creep**: –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ milestones —Å acceptance criteria
- **Resource constraints**: Prioritization matrix
- **Timeline delays**: Agile —Å 2-week sprints

## üìä –£—Å–ø–µ—Ö –ú–µ—Ç—Ä–∏–∫–∏

### **–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ**
- **Stability**: 98% successful interactions
- **Performance**: <100ms latency, <100MB memory
- **Compatibility**: 95% browser/device coverage

### **–ë–∏–∑–Ω–µ—Å**
- **User Satisfaction**: NPS >70
- **Retention**: 60% monthly retention
- **Support Tickets**: <5% of users

### **–†–∞–∑—Ä–∞–±–æ—Ç–∫–∞**
- **Code Coverage**: 80% automated tests
- **Deployment Success**: 95% successful deployments
- **Technical Debt**: <10% ratio
