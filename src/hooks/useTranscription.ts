/**
 * useTranscription - –ì–æ–ª–æ—Å–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å blob-based VAD
 * 
 * –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:
 * - iOS/Android: OpenAI Whisper + VAD –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ blob'–æ–≤ (—Ä–∞–±–æ—Ç–∞–µ—Ç!)
 * - Desktop: Browser SpeechRecognition + OpenAI fallback
 * 
 * –ö–ª—é—á–µ–≤–æ–µ –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Å—Ç–∞—Ä–æ–π —Å–∏—Å—Ç–µ–º—ã:
 * - –ê–Ω–∞–ª–∏–∑ –≥—Ä–æ–º–∫–æ—Å—Ç–∏ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –ø–æ –∑–∞–ø–∏—Å–∞–Ω–Ω—ã–º blob'–∞–º (decodeAudioData)
 * - –ê –ù–ï —á–µ—Ä–µ–∑ AnalyserNode –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ (–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ iOS)
 */

import { useState, useRef, useEffect, useCallback } from 'react';
import { psychologistAI } from '@/services/openai';

interface UseTranscriptionProps {
  onTranscriptionComplete: (text: string, source: 'browser' | 'openai' | 'manual') => void;
  onSpeechStart?: () => void;
  onInterruption?: () => void;
  isTTSActiveRef: React.MutableRefObject<boolean>;
  onError?: (error: string) => void;
  addDebugLog?: (message: string) => void;
}

// –†–∞—Å—à–∏—Ä—è–µ–º window –¥–ª—è deviceDebugLogged
declare global {
  interface Window {
    deviceDebugLogged?: boolean;
  }
}

export const useTranscription = ({
  onTranscriptionComplete,
  onSpeechStart,
  onInterruption,
  isTTSActiveRef,
  onError,
  addDebugLog = console.log
}: UseTranscriptionProps) => {
  // === STATE ===
  const [transcriptionStatus, setTranscriptionStatus] = useState<string | null>(null);
  const [isIOS, setIsIOS] = useState(false);
  const [forceOpenAI, setForceOpenAI] = useState(false);
  const [transcriptionMode, setTranscriptionMode] = useState<'browser' | 'openai'>('browser');
  const [microphoneAccessGranted, setMicrophoneAccessGranted] = useState(false);
  const [microphonePermissionStatus, setMicrophonePermissionStatus] = useState<'unknown' | 'granted' | 'denied' | 'prompt'>('unknown');

  // === REFS ===
  const recognitionRef = useRef<any>(null);
  const recognitionActiveRef = useRef(false);
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const recordedChunksRef = useRef<Blob[]>([]);
  const audioStreamRef = useRef<MediaStream | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const volumeMonitorRef = useRef<number | null>(null);
  const speechTimeoutRef = useRef<number | null>(null);
  const browserRetryCountRef = useRef(0);
  const lastProcessedTextRef = useRef<string>('');

  // Mobile VAD refs
  const mobileVADIntervalRef = useRef<number | null>(null);
  const speechActiveRef = useRef(false);
  const silenceStartTimeRef = useRef<number>(0);
  const speechStartTimeRef = useRef<number>(0);
  const speechChunksRef = useRef<Blob[]>([]);
  const isProcessingRef = useRef(false);
  const lastChunkIndexRef = useRef(0);

  // Safari interruption state
  const safariSpeechCountRef = useRef(0);
  const lastSafariSpeechTimeRef = useRef(0);

  // === CONSTANTS ===
  const SAFARI_VOICE_THRESHOLD = 60;
  const SAFARI_CONFIRMATION_FRAMES = 3;
  const SAFARI_DEBOUNCE = 1000;

  // Mobile VAD constants - –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
  const MOBILE_VAD_INTERVAL = 400;        // –ê–Ω–∞–ª–∏–∑ –∫–∞–∂–¥—ã–µ 400ms
  const MOBILE_SPEECH_THRESHOLD = 1.0;    // 1.0% –≥—Ä–æ–º–∫–æ—Å—Ç–∏ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–µ—á–∏ (—á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–π)
  const MOBILE_SILENCE_DURATION = 1200;   // 1.2 —Å–µ–∫ —Ç–∏—à–∏–Ω—ã –¥–ª—è –æ–∫–æ–Ω—á–∞–Ω–∏—è —Ä–µ—á–∏
  const MOBILE_MIN_SPEECH_DURATION = 400; // –ú–∏–Ω–∏–º—É–º 400ms —Ä–µ—á–∏
  const MOBILE_MIN_AUDIO_SIZE = 4000;     // –ú–∏–Ω–∏–º—É–º 4KB –∞—É–¥–∏–æ

  // === BROWSER DETECTION ===
  const isIOSDevice = useCallback(() => {
    const ua = navigator.userAgent.toLowerCase();
    return /iphone|ipad|ipod/.test(ua) || 
           (navigator.platform === 'MacIntel' && navigator.maxTouchPoints > 1);
  }, []);

  const isAndroidDevice = useCallback(() => {
    return /android/.test(navigator.userAgent.toLowerCase());
  }, []);

  const isMobileDevice = useCallback(() => {
    return /android|webos|iphone|ipad|ipod|blackberry|iemobile|opera mini/i.test(
      navigator.userAgent.toLowerCase()
    );
  }, []);

  const hasEchoProblems = useCallback(() => {
    return /chrome|chromium|edg\/|opera|brave/.test(navigator.userAgent.toLowerCase());
  }, []);

  // === HALLUCINATION FILTER ===
  const filterHallucinatedText = useCallback((text: string): string | null => {
    if (!text) return null;
    
    const lowerText = text.toLowerCase();

    // –ü–∞—Ç—Ç–µ—Ä–Ω—ã –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π Whisper
    const hallucinationPatterns = [
      /–ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Å–ª–µ–¥—É–µ—Ç/i,
      /—Å –≤–∞–º–∏ –±—ã–ª/i,
      /–¥–æ —Å–≤–∏–¥–∞–Ω–∏—è/i,
      /–¥–æ –Ω–æ–≤—ã—Ö –≤—Å—Ç—Ä–µ—á/i,
      /—Å–ø–∞—Å–∏–±–æ –∑–∞ –≤–Ω–∏–º–∞–Ω–∏–µ/i,
      /–∫–æ–Ω–µ—Ü$/i,
      /–∑–∞–∫–æ–Ω—á–∏–ª–∏/i,
      /—Å—É–±—Ç–∏—Ç—Ä—ã/i,
      /–ø–æ–¥–ø–∏—Å—ã–≤–∞–π—Ç–µ—Å—å/i,
      /—Å—Ç–∞–≤—å—Ç–µ –ª–∞–π–∫/i,
      /–±–ª–∞–≥–æ–¥–∞—Ä—é –∑–∞ –ø—Ä–æ—Å–º–æ—Ç—Ä/i,
      /^\s*\.+\s*$/,  // –¢–æ–ª—å–∫–æ —Ç–æ—á–∫–∏
      /^\s*,+\s*$/,   // –¢–æ–ª—å–∫–æ –∑–∞–ø—è—Ç—ã–µ
    ];

    for (const pattern of hallucinationPatterns) {
      if (pattern.test(lowerText)) {
        addDebugLog(`[Filter] ‚ö†Ô∏è Hallucination detected: "${text}"`);
        return null;
      }
    }

    // –§–∏–ª—å—Ç—Ä –ø–æ –¥–ª–∏–Ω–µ
    if (text.length > 200) {
      addDebugLog(`[Filter] ‚ö†Ô∏è Too long (${text.length} chars): "${text.substring(0, 50)}..."`);
      return null;
    }
    
    if (text.length < 2) {
      addDebugLog(`[Filter] ‚ö†Ô∏è Too short (${text.length} chars)`);
      return null;
    }

    // –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
    if (text.split(/[.!?]/).filter(s => s.trim()).length > 4) {
      addDebugLog(`[Filter] ‚ö†Ô∏è Too many sentences`);
      return null;
    }

    // –ë–µ—Å—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ –∑–≤—É–∫–∏
    const meaninglessPatterns = [
      /^[–∞-—èa-z]{1}$/i,
      /^[—ç—ç—ç]+$/i,
      /^[–º–º–º]+$/i,
      /^[–∞–∞–∞]+$/i,
      /^[—É—É—É]+$/i,
      /^[–æ–æ–æ]+$/i,
      /^[–∞-—èa-z]{1,2}$/i,
    ];

    for (const pattern of meaninglessPatterns) {
      if (pattern.test(text.trim())) {
        addDebugLog(`[Filter] ‚ö†Ô∏è Meaningless sound: "${text}"`);
        return null;
      }
    }

    return text;
  }, [addDebugLog]);

  // === MICROPHONE PERMISSIONS ===
  const checkMicrophonePermissions = useCallback(async () => {
    if (!navigator.permissions?.query) return;
    
    try {
      const result = await navigator.permissions.query({ name: 'microphone' as PermissionName });
      setMicrophonePermissionStatus(result.state);
      result.addEventListener('change', () => setMicrophonePermissionStatus(result.state));
    } catch (error) {
      addDebugLog(`[Permissions] Could not query: ${error}`);
    }
  }, [addDebugLog]);

  // === AUDIO VOLUME CHECK (BLOB-BASED) ===
  // –≠—Ç–æ –∫–ª—é—á–µ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è! –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≥—Ä–æ–º–∫–æ—Å—Ç—å –ø–æ –∑–∞–ø–∏—Å–∞–Ω–Ω–æ–º—É blob'—É
  // –†–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ iOS –≤ –æ—Ç–ª–∏—á–∏–µ –æ—Ç AnalyserNode.getByteFrequencyData()
  const checkAudioVolume = useCallback(async (audioBlob: Blob): Promise<number> => {
    try {
      const AudioContextClass = window.AudioContext || (window as any).webkitAudioContext;
      const tempContext = new AudioContextClass();
      
      const arrayBuffer = await audioBlob.arrayBuffer();
      const audioBuffer = await tempContext.decodeAudioData(arrayBuffer);
      
      let sum = 0;
      let count = 0;
      
      for (let channel = 0; channel < audioBuffer.numberOfChannels; channel++) {
        const channelData = audioBuffer.getChannelData(channel);
        for (let i = 0; i < channelData.length; i++) {
          sum += Math.abs(channelData[i]);
          count++;
        }
      }
      
      await tempContext.close();
      return (sum / count) * 100; // –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö
    } catch (error) {
      // –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º 0
      return 0;
    }
  }, []);

  // === OPENAI TRANSCRIPTION ===
  const transcribeWithOpenAI = useCallback(async (audioBlob: Blob): Promise<string | null> => {
    try {
      addDebugLog(`[OpenAI] üé§ Transcribing ${audioBlob.size} bytes...`);
      setTranscriptionStatus("–†–∞—Å–ø–æ–∑–Ω–∞—é —Ä–µ—á—å...");

      const text = await psychologistAI.transcribeAudio(audioBlob);

      if (text?.trim()) {
        addDebugLog(`[OpenAI] ‚úÖ Result: "${text.substring(0, 60)}..."`);
        return text.trim();
      }
      
      addDebugLog(`[OpenAI] ‚ö†Ô∏è Empty result`);
      return null;
    } catch (error: any) {
      addDebugLog(`[OpenAI] ‚ùå Error: ${error.message}`);
      return null;
    } finally {
      setTranscriptionStatus(null);
    }
  }, [addDebugLog]);

  // === MEDIA RECORDER ===
  const startMediaRecording = useCallback((stream: MediaStream) => {
    if (mediaRecorderRef.current) return;

    try {
      const mimeTypes = [
        'audio/webm;codecs=opus',
        'audio/webm',
        'audio/mp4',
        'audio/wav',
        'audio/ogg'
      ];
      const selectedMimeType = mimeTypes.find(type => MediaRecorder.isTypeSupported(type));

      if (!selectedMimeType) {
        addDebugLog(`[MediaRec] ‚ùå No supported format`);
        return;
      }

      addDebugLog(`[MediaRec] Using format: ${selectedMimeType}`);

      const recorder = new MediaRecorder(stream, { mimeType: selectedMimeType });
      mediaRecorderRef.current = recorder;
      recordedChunksRef.current = [];
      lastChunkIndexRef.current = 0;

      recorder.ondataavailable = (e) => {
        if (e.data.size > 0) {
          recordedChunksRef.current.push(e.data);
        }
      };

      recorder.onerror = (event: any) => {
        addDebugLog(`[MediaRec] ‚ùå Error: ${event.error?.message || 'Unknown'}`);
      };

      // –ó–∞–ø–∏—Å—ã–≤–∞–µ–º chunks —Å –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–º –¥–ª—è VAD –∞–Ω–∞–ª–∏–∑–∞
      recorder.start(MOBILE_VAD_INTERVAL);
      addDebugLog(`[MediaRec] ‚úÖ Started (${MOBILE_VAD_INTERVAL}ms chunks)`);
    } catch (error: any) {
      addDebugLog(`[MediaRec] ‚ùå Start failed: ${error.message}`);
    }
  }, [addDebugLog, MOBILE_VAD_INTERVAL]);

  const stopMediaRecording = useCallback(async (): Promise<Blob | null> => {
    return new Promise((resolve) => {
      if (!mediaRecorderRef.current || mediaRecorderRef.current.state === 'inactive') {
        resolve(null);
        return;
      }

      const recorder = mediaRecorderRef.current;
      
      recorder.onstop = () => {
        const blob = new Blob(recordedChunksRef.current, {
          type: recorder.mimeType || 'audio/webm'
        });
        recordedChunksRef.current = [];
        mediaRecorderRef.current = null;
        resolve(blob);
      };

      recorder.stop();
    });
  }, []);

  // === MOBILE VAD (BLOB-BASED) ===
  // –ì–ª–∞–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–µ—á–∏ –¥–ª—è –º–æ–±–∏–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤
  const startMobileVAD = useCallback(() => {
    if (mobileVADIntervalRef.current) return;

    addDebugLog(`[MobileVAD] üé§ Starting blob-based voice detection`);
    addDebugLog(`[MobileVAD] Settings: threshold=${MOBILE_SPEECH_THRESHOLD}%, silence=${MOBILE_SILENCE_DURATION}ms`);

    mobileVADIntervalRef.current = window.setInterval(async () => {
      // –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ TTS –∞–∫—Ç–∏–≤–µ–Ω (—ç—Ö–æ-–ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ)
      if (isTTSActiveRef.current) {
        if (speechActiveRef.current) {
          addDebugLog(`[MobileVAD] üîá TTS active - clearing speech buffer`);
          speechChunksRef.current = [];
          speechActiveRef.current = false;
          silenceStartTimeRef.current = 0;
        }
        return;
      }

      // –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ —É–∂–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â—É—é —Ä–µ—á—å
      if (isProcessingRef.current) return;

      // –ü–æ–ª—É—á–∞–µ–º –Ω–æ–≤—ã–µ chunks —Å –º–æ–º–µ–Ω—Ç–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–π –ø—Ä–æ–≤–µ—Ä–∫–∏
      const currentChunks = recordedChunksRef.current;
      const newChunks = currentChunks.slice(lastChunkIndexRef.current);
      lastChunkIndexRef.current = currentChunks.length;

      if (newChunks.length === 0) return;

      // –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≥—Ä–æ–º–∫–æ—Å—Ç—å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ chunk
      const latestChunk = newChunks[newChunks.length - 1];
      const volumeLevel = await checkAudioVolume(latestChunk);

      const now = Date.now();
      const isSpeaking = volumeLevel > MOBILE_SPEECH_THRESHOLD;

      // –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∂–¥—É—é —Å–µ–∫—É–Ω–¥—É –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
      if (Math.floor(now / 1000) !== Math.floor((now - MOBILE_VAD_INTERVAL) / 1000)) {
        addDebugLog(`[MobileVAD] üìä Vol: ${volumeLevel.toFixed(2)}% | Speaking: ${isSpeaking} | Active: ${speechActiveRef.current}`);
      }

      if (isSpeaking) {
        // === –û–ë–ù–ê–†–£–ñ–ï–ù–ê –†–ï–ß–¨ ===
        if (!speechActiveRef.current) {
          addDebugLog(`[MobileVAD] üé§ Speech STARTED (vol: ${volumeLevel.toFixed(2)}%)`);
          speechActiveRef.current = true;
          speechStartTimeRef.current = now;
          speechChunksRef.current = [];
          onSpeechStart?.();
        }
        
        // –î–æ–±–∞–≤–ª—è–µ–º chunks –≤ –±—É—Ñ–µ—Ä —Ä–µ—á–∏
        speechChunksRef.current.push(...newChunks);
        silenceStartTimeRef.current = 0; // –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ —Ç–∏—à–∏–Ω—ã
        
      } else {
        // === –¢–ò–®–ò–ù–ê ===
        if (speechActiveRef.current) {
          // –î–æ–±–∞–≤–ª—è–µ–º —Ç–∏—Ö–∏–µ chunks (–º–æ–≥—É—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–æ–Ω–µ—Ü —Å–ª–æ–≤–∞)
          speechChunksRef.current.push(...newChunks);
          
          if (!silenceStartTimeRef.current) {
            silenceStartTimeRef.current = now;
            addDebugLog(`[MobileVAD] üîá Silence started, waiting ${MOBILE_SILENCE_DURATION}ms...`);
          }
          
          const silenceDuration = now - silenceStartTimeRef.current;
          const speechDuration = now - speechStartTimeRef.current;
          
          // –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ª–∏ –¥–ª–∏–Ω–Ω–∞—è —Ç–∏—à–∏–Ω–∞
          if (silenceDuration >= MOBILE_SILENCE_DURATION) {
            addDebugLog(`[MobileVAD] ‚úÖ Speech ENDED (duration: ${speechDuration}ms, silence: ${silenceDuration}ms)`);
            
            // –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ
            speechActiveRef.current = false;
            silenceStartTimeRef.current = 0;
            
            // –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ä–µ—á–∏
            if (speechDuration < MOBILE_MIN_SPEECH_DURATION) {
              addDebugLog(`[MobileVAD] ‚ö†Ô∏è Speech too short (${speechDuration}ms < ${MOBILE_MIN_SPEECH_DURATION}ms), skipping`);
              speechChunksRef.current = [];
              return;
            }
            
            // –°–æ–∑–¥–∞–µ–º blob –∏–∑ –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã—Ö chunks
            if (speechChunksRef.current.length > 0) {
              const speechBlob = new Blob(speechChunksRef.current, { type: 'audio/webm' });
              speechChunksRef.current = [];
              
              // –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä
              if (speechBlob.size < MOBILE_MIN_AUDIO_SIZE) {
                addDebugLog(`[MobileVAD] ‚ö†Ô∏è Audio too small (${speechBlob.size} < ${MOBILE_MIN_AUDIO_SIZE} bytes), skipping`);
                return;
              }
              
              // –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –≥—Ä–æ–º–∫–æ—Å—Ç–∏ –≤—Å–µ–≥–æ –∞—É–¥–∏–æ
              const finalVolume = await checkAudioVolume(speechBlob);
              
              if (finalVolume < MOBILE_SPEECH_THRESHOLD * 0.5) {
                addDebugLog(`[MobileVAD] ‚ö†Ô∏è Final volume too low (${finalVolume.toFixed(2)}%), skipping`);
                return;
              }
              
              // === –û–¢–ü–†–ê–í–õ–Ø–ï–ú –ù–ê –¢–†–ê–ù–°–ö–†–ò–ë–ê–¶–ò–Æ ===
              addDebugLog(`[MobileVAD] üì§ Sending ${speechBlob.size} bytes (vol: ${finalVolume.toFixed(2)}%)`);
              
              isProcessingRef.current = true;
              
              try {
                const text = await transcribeWithOpenAI(speechBlob);
                
                if (text?.trim()) {
            const filteredText = filterHallucinatedText(text.trim());
                  
            if (filteredText) {
                    addDebugLog(`[MobileVAD] ‚úÖ Transcribed: "${filteredText}"`);
              onTranscriptionComplete(filteredText, 'openai');
                  }
                }
              } catch (error: any) {
                addDebugLog(`[MobileVAD] ‚ùå Transcription error: ${error.message}`);
              } finally {
                isProcessingRef.current = false;
              }
            }
          }
        }
      }
    }, MOBILE_VAD_INTERVAL);
  }, [
    checkAudioVolume, 
    transcribeWithOpenAI, 
    filterHallucinatedText, 
    onTranscriptionComplete, 
    onSpeechStart, 
    isTTSActiveRef, 
    addDebugLog,
    MOBILE_VAD_INTERVAL,
    MOBILE_SPEECH_THRESHOLD,
    MOBILE_SILENCE_DURATION,
    MOBILE_MIN_SPEECH_DURATION,
    MOBILE_MIN_AUDIO_SIZE
  ]);

  const stopMobileVAD = useCallback(() => {
    if (mobileVADIntervalRef.current) {
      addDebugLog(`[MobileVAD] üõë Stopping`);
      clearInterval(mobileVADIntervalRef.current);
      mobileVADIntervalRef.current = null;
    }
    speechActiveRef.current = false;
    speechChunksRef.current = [];
    silenceStartTimeRef.current = 0;
  }, [addDebugLog]);

  // === VOLUME MONITORING FOR DESKTOP (Safari interruption) ===
  const startVolumeMonitoring = useCallback(async (stream: MediaStream) => {
    // –¢–æ–ª—å–∫–æ –¥–ª—è –¥–µ—Å–∫—Ç–æ–ø–∞ - –Ω–∞ –º–æ–±–∏–ª—å–Ω—ã—Ö –∏—Å–ø–æ–ª—å–∑—É–µ–º blob-based VAD
    if (isMobileDevice()) return;

    try {
      const AudioContextClass = window.AudioContext || (window as any).webkitAudioContext;
      audioContextRef.current = new AudioContextClass();
      
      if (audioContextRef.current.state === 'suspended') {
        await audioContextRef.current.resume();
      }

      const source = audioContextRef.current.createMediaStreamSource(stream);
      const analyser = audioContextRef.current.createAnalyser();
      analyser.fftSize = 256;
      source.connect(analyser);

      const dataArray = new Uint8Array(analyser.frequencyBinCount);
      
      const checkVolume = () => {
        if (!recognitionActiveRef.current) return;
        
          analyser.getByteFrequencyData(dataArray);
          const average = dataArray.reduce((a, b) => a + b, 0) / dataArray.length;

        // Safari interruption logic
        if (!hasEchoProblems()) {
          const threshold = isTTSActiveRef.current 
            ? SAFARI_VOICE_THRESHOLD + 15 
            : SAFARI_VOICE_THRESHOLD;

          if (average > threshold) {
            safariSpeechCountRef.current++;
            if (safariSpeechCountRef.current >= SAFARI_CONFIRMATION_FRAMES) {
              const now = Date.now();
              if (now - lastSafariSpeechTimeRef.current > SAFARI_DEBOUNCE) {
                lastSafariSpeechTimeRef.current = now;
                  onInterruption?.();
                safariSpeechCountRef.current = 0;
              }
            }
          } else {
            safariSpeechCountRef.current = 0;
          }
        }
        
        volumeMonitorRef.current = requestAnimationFrame(checkVolume);
      };
      
      volumeMonitorRef.current = requestAnimationFrame(checkVolume);
      addDebugLog(`[Volume] ‚úÖ Desktop monitoring started`);
    } catch (error: any) {
      addDebugLog(`[Volume] ‚ùå Failed: ${error.message}`);
    }
  }, [hasEchoProblems, isMobileDevice, isTTSActiveRef, onInterruption, addDebugLog]);

  const stopVolumeMonitoring = useCallback(() => {
    if (volumeMonitorRef.current) {
      cancelAnimationFrame(volumeMonitorRef.current);
      volumeMonitorRef.current = null;
    }
    if (audioContextRef.current) {
      audioContextRef.current.close().catch(() => {});
      audioContextRef.current = null;
    }
  }, []);

  // === MAIN INITIALIZATION ===
  const initializeRecognition = useCallback(async () => {
    addDebugLog(`[Init] üöÄ Starting recognition initialization...`);

    await checkMicrophonePermissions();
    lastProcessedTextRef.current = '';

    // Device detection
    const ios = isIOSDevice();
    const android = isAndroidDevice();
    const mobile = isMobileDevice();
    setIsIOS(ios);

    // API support check
    const speechRecognitionSupport = !!(window as any).SpeechRecognition || 
                                      !!(window as any).webkitSpeechRecognition;

    addDebugLog(`[Device] iOS: ${ios}, Android: ${android}, Mobile: ${mobile}`);
    addDebugLog(`[API] SpeechRecognition: ${speechRecognitionSupport}`);

    // Determine strategy
    const shouldForceOpenAI = ios || android || !speechRecognitionSupport;
    
    addDebugLog(`[Strategy] ${shouldForceOpenAI ? 'üì± OpenAI Mode (Mobile VAD)' : 'üíª Browser Mode'}`);

    setForceOpenAI(shouldForceOpenAI);
    if (shouldForceOpenAI) setTranscriptionMode('openai');

    // Get microphone access
    try {
      const constraints = mobile ? {
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          sampleRate: { ideal: 44100 },
          channelCount: { ideal: 1 }
        }
      } : { audio: true };

      addDebugLog(`[Mic] üé§ Requesting access...`);
      const stream = await navigator.mediaDevices.getUserMedia(constraints);
      
      // Log track info
      const tracks = stream.getAudioTracks();
      addDebugLog(`[Mic] ‚úÖ Access granted (${tracks.length} tracks)`);
      tracks.forEach((track, i) => {
        addDebugLog(`[Mic] Track ${i}: ${track.label}, enabled=${track.enabled}, state=${track.readyState}`);
      });

      audioStreamRef.current = stream;
      setMicrophoneAccessGranted(true);

      // Start recording
      startMediaRecording(stream);

      // === MOBILE: Use blob-based VAD ===
      if (ios || android) {
        addDebugLog(`[Init] üì± Starting Mobile VAD system`);
        startMobileVAD();
        recognitionActiveRef.current = true;
        addDebugLog(`[Init] ‚úÖ Mobile VAD active - speak to test!`);
        return;
      }

      // === DESKTOP: Volume monitoring + Browser Recognition ===
      startVolumeMonitoring(stream);

      if (!shouldForceOpenAI) {
        addDebugLog(`[Init] üíª Starting Browser SpeechRecognition`);
        
        const SpeechRecognition = (window as any).SpeechRecognition || 
                                  (window as any).webkitSpeechRecognition;
      const recognition = new SpeechRecognition();
        
      recognition.lang = "ru-RU";
      recognition.continuous = true;
      recognition.interimResults = true;
      recognition.maxAlternatives = 1;

      recognition.onresult = (event: any) => {
          // Echo prevention for Chrome
        if (hasEchoProblems() && isTTSActiveRef.current) return;

        let finalTranscript = "";
        let interimTranscript = "";

        for (let i = event.resultIndex; i < event.results.length; i++) {
          const result = event.results[i];
            if (result.isFinal) {
              finalTranscript += result[0].transcript;
            } else {
              interimTranscript += result[0].transcript;
            }
        }

        if (finalTranscript.trim()) {
          const trimmedText = finalTranscript.trim();
          const lastText = lastProcessedTextRef.current;

            // Dedupe logic
            const isExtension = lastText && 
                               trimmedText.startsWith(lastText) && 
                               (trimmedText.length - lastText.length) > 5;
          const lengthDiff = Math.abs(trimmedText.length - (lastText?.length || 0));
          const maxLength = Math.max(trimmedText.length, lastText?.length || 0);
          const isMinorCorrection = lastText && (lengthDiff / maxLength) < 0.2 && lengthDiff < 50;

          if (isExtension || isMinorCorrection || lastProcessedTextRef.current === trimmedText) {
            lastProcessedTextRef.current = trimmedText;
            return;
          }

          lastProcessedTextRef.current = trimmedText;
          if (speechTimeoutRef.current) clearTimeout(speechTimeoutRef.current);
          browserRetryCountRef.current = 0;
            
            addDebugLog(`[Browser] ‚úÖ Final: "${trimmedText}"`);
          onTranscriptionComplete(trimmedText, 'browser');
            
        } else if (interimTranscript.trim()) {
          if (speechTimeoutRef.current) clearTimeout(speechTimeoutRef.current);
            
          speechTimeoutRef.current = window.setTimeout(() => {
            if (hasEchoProblems() && isTTSActiveRef.current) return;
              
            const trimmedInterim = interimTranscript.trim();
              addDebugLog(`[Browser] ‚è±Ô∏è Interim timeout: "${trimmedInterim}"`);
            onTranscriptionComplete(trimmedInterim, 'browser');
          }, 1500);
        }
      };

      recognition.onspeechstart = () => {
        lastProcessedTextRef.current = '';
        onSpeechStart?.();
          
          // Safari interruption
        if (!hasEchoProblems() && isTTSActiveRef.current) {
            const now = Date.now();
            if (now - lastSafariSpeechTimeRef.current > SAFARI_DEBOUNCE) {
              lastSafariSpeechTimeRef.current = now;
            onInterruption?.();
          }
        }
      };

      recognition.onerror = async (event: any) => {
        if (event.error === 'no-speech' || event.error === 'aborted') return;
          
          addDebugLog(`[Browser] ‚ùå Error: ${event.error}`);

        const retryable = ['network', 'audio-capture', 'not-allowed'];
        if (retryable.includes(event.error) && browserRetryCountRef.current < 3) {
          browserRetryCountRef.current++;
          setTimeout(() => {
            if (recognitionActiveRef.current) {
              try { recognition.start(); } catch(e) {}
            }
          }, 1000 * browserRetryCountRef.current);
          return;
        }

          // Fallback to OpenAI
        if (browserRetryCountRef.current >= 3 || ['network', 'audio-capture'].includes(event.error)) {
            addDebugLog(`[Fallback] Switching to OpenAI`);
          setTranscriptionMode('openai');
            
          const blob = await stopMediaRecording();
          if (blob && blob.size > 1000) {
            const text = await transcribeWithOpenAI(blob);
            if (text) {
                const filtered = filterHallucinatedText(text);
                if (filtered) {
                  onTranscriptionComplete(filtered, 'openai');
                }
            } else {
              onError?.("–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ä–µ—á—å");
            }
          }
            
          setTranscriptionMode('browser');
          browserRetryCountRef.current = 0;
            
            // Restart recording
            if (audioStreamRef.current) {
              startMediaRecording(audioStreamRef.current);
            }
        }
      };

      recognition.onend = () => {
        if (recognitionActiveRef.current && !isTTSActiveRef.current) {
          try { recognition.start(); } catch (e) {}
        }
      };

      recognitionRef.current = recognition;
      recognitionActiveRef.current = true;
      recognition.start();
        
        addDebugLog(`[Init] ‚úÖ Browser recognition started`);
      }

    } catch (error: any) {
      addDebugLog(`[Mic] ‚ùå Failed: ${error.name} - ${error.message}`);

      let errorMessage = "–û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞ –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É";
      
      switch (error.name) {
        case 'NotAllowedError':
          errorMessage = "–î–æ—Å—Ç—É–ø –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É –∑–∞–ø—Ä–µ—â–µ–Ω. –†–∞–∑—Ä–µ—à–∏—Ç–µ –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö –±—Ä–∞—É–∑–µ—Ä–∞.";
          break;
        case 'NotFoundError':
        errorMessage = "–ú–∏–∫—Ä–æ—Ñ–æ–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω.";
          break;
        case 'NotReadableError':
          errorMessage = "–ú–∏–∫—Ä–æ—Ñ–æ–Ω –∑–∞–Ω—è—Ç –¥—Ä—É–≥–∏–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ–º.";
          break;
        case 'SecurityError':
        errorMessage = "–¢—Ä–µ–±—É–µ—Ç—Å—è HTTPS –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É.";
          break;
      }

      onError?.(errorMessage);
      setMicrophoneAccessGranted(false);
    }
  }, [
    checkMicrophonePermissions,
    isIOSDevice,
    isAndroidDevice,
    isMobileDevice,
    hasEchoProblems,
    startMediaRecording,
    stopMediaRecording,
    startMobileVAD,
    startVolumeMonitoring,
    transcribeWithOpenAI,
    filterHallucinatedText,
    onTranscriptionComplete,
    onSpeechStart,
    onInterruption,
    onError,
    isTTSActiveRef,
    addDebugLog
  ]);

  // === CLEANUP ===
  const cleanup = useCallback(() => {
    addDebugLog(`[Cleanup] üßπ Cleaning up...`);
    
    lastProcessedTextRef.current = '';
    recognitionActiveRef.current = false;
    speechActiveRef.current = false;
    isProcessingRef.current = false;
    
    if (recognitionRef.current) {
      try { recognitionRef.current.stop(); } catch(e) {}
      recognitionRef.current = null;
    }
    
    stopVolumeMonitoring();
    stopMobileVAD();
    
    if (mediaRecorderRef.current && mediaRecorderRef.current.state !== 'inactive') {
      try { mediaRecorderRef.current.stop(); } catch(e) {}
    }
    mediaRecorderRef.current = null;
    recordedChunksRef.current = [];
    
    if (audioStreamRef.current) {
      audioStreamRef.current.getTracks().forEach(t => t.stop());
      audioStreamRef.current = null;
    }
    
    if (speechTimeoutRef.current) {
      clearTimeout(speechTimeoutRef.current);
      speechTimeoutRef.current = null;
    }
    
    addDebugLog(`[Cleanup] ‚úÖ Done`);
  }, [stopVolumeMonitoring, stopMobileVAD, addDebugLog]);

  // Cleanup on unmount
  useEffect(() => {
    return cleanup;
  }, [cleanup]);

  // === RETURN ===
  return {
    initializeRecognition,
    cleanup,
    transcriptionStatus,
    microphoneAccessGranted,
    microphonePermissionStatus,
    isIOS,
    forceOpenAI,
    transcriptionMode,
    stopRecognition: useCallback(() => {
      addDebugLog(`[Recognition] üõë Stopping`);
      recognitionActiveRef.current = false;
      if (recognitionRef.current) {
        try { recognitionRef.current.stop(); } catch(e) {}
      }
      stopMobileVAD();
    }, [stopMobileVAD, addDebugLog]),
    startRecognition: useCallback(() => {
      addDebugLog(`[Recognition] ‚ñ∂Ô∏è Starting`);
      recognitionActiveRef.current = true;
      if (recognitionRef.current) {
        try { recognitionRef.current.start(); } catch(e) {}
      }
      if (isMobileDevice() && audioStreamRef.current) {
        startMobileVAD();
    }
    }, [startMobileVAD, isMobileDevice, addDebugLog])
  };
};
