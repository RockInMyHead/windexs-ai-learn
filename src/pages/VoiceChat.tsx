import Navigation from "@/components/Navigation";
import { useParams } from "react-router-dom";
import { getCourseDisplayName } from "@/lib/utils";
import { Mic, MicOff, Volume2, VolumeX, Phone, PhoneOff, Loader2 } from "lucide-react";
import { Button } from "@/components/ui/button";
import { Card, CardContent } from "@/components/ui/card";
import { Badge } from "@/components/ui/badge";
import { useState, useRef, useCallback, useEffect } from "react";
import { useAuth } from "@/contexts/AuthContext";
import { useToast } from "@/hooks/use-toast";

// Web Speech API types
interface SpeechRecognitionEvent extends Event {
  results: SpeechRecognitionResultList;
}

interface SpeechRecognitionErrorEvent extends Event {
  error: string;
  message?: string;
}

interface SpeechRecognition extends EventTarget {
  continuous: boolean;
  interimResults: boolean;
  lang: string;
  maxAlternatives: number;
  start(): void;
  stop(): void;
  abort(): void;
  onstart: ((event: Event) => void) | null;
  onresult: ((event: SpeechRecognitionEvent) => void) | null;
  onerror: ((event: SpeechRecognitionErrorEvent) => void) | null;
  onend: ((event: Event) => void) | null;
}

declare global {
  interface Window {
    SpeechRecognition?: new () => SpeechRecognition;
    webkitSpeechRecognition?: new () => SpeechRecognition;
    mozSpeechRecognition?: new () => SpeechRecognition; // Firefox support
  }
}

const VoiceChat = () => {
  const { courseId } = useParams();
  const { token } = useAuth();
  const { toast } = useToast();

  const [isRecording, setIsRecording] = useState(false);
  const [isTranscribing, setIsTranscribing] = useState(false);
  const [isGeneratingResponse, setIsGeneratingResponse] = useState(false);
  const [isSpeaking, setIsSpeaking] = useState(false);
  const [isMicEnabled, setIsMicEnabled] = useState(true);
  const [isSoundEnabled, setIsSoundEnabled] = useState(true);
  const [userProfile, setUserProfile] = useState<any>(null);
  const [useFallbackTranscription, setUseFallbackTranscription] = useState(false);

  const speechRecognitionRef = useRef<SpeechRecognition | null>(null);
  const currentAudioRef = useRef<HTMLAudioElement | null>(null);
  const lastTranscriptRef = useRef<string>('');
  const currentTTSTextRef = useRef<string>(''); // Store current TTS text to detect echo
  
  // Fallback recording refs (for browsers without Web Speech API)
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioChunksRef = useRef<Blob[]>([]);
  const mediaStreamRef = useRef<MediaStream | null>(null);

  // Function to check if recognized text is an echo of the TTS output
  const isEchoOfTTS = (recognizedText: string): boolean => {
    if (!currentTTSTextRef.current || !isSpeaking) {
      console.log('üîá isEchoOfTTS: TTS text is empty or not speaking', {
        hasTTS: !!currentTTSTextRef.current,
        isSpeaking,
        recognizedText
      });
      return false;
    }

    const normalizedRecognized = recognizedText.toLowerCase().trim();
    const normalizedTTS = currentTTSTextRef.current.toLowerCase();

    console.log('üîç Checking for TTS echo:', {
      recognized: normalizedRecognized,
      tts: normalizedTTS.substring(0, 100) + '...',
      isSpeaking
    });

    // More aggressive echo detection: check if recognized text appears anywhere in TTS
    if (normalizedRecognized.length > 3) {
      // Remove punctuation and extra spaces for better matching
      const cleanRecognized = normalizedRecognized.replace(/[^\w\s]/g, '').replace(/\s+/g, ' ');
      const cleanTTS = normalizedTTS.replace(/[^\w\s]/g, '').replace(/\s+/g, ' ');

      // Check if recognized text is contained in TTS text (with fuzzy matching)
      if (cleanTTS.includes(cleanRecognized)) {
        console.log('üîá –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ —Ñ—Ä–∞–∑–∞ —ç—Ö–æ TTS, –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º:', normalizedRecognized);
        return true;
      }

      // Check if most words from recognized text appear in TTS
      const recognizedWords = cleanRecognized.split(/\s+/);
      const ttsWords = cleanTTS.split(/\s+/);
      let matchingWords = 0;

      for (const word of recognizedWords) {
        if (word.length > 2 && ttsWords.includes(word)) {
          matchingWords++;
        }
      }

      // If 70% or more words match, it's likely echo
      const matchRatio = matchingWords / recognizedWords.length;
      if (matchRatio >= 0.7 && recognizedWords.length >= 3) {
        console.log('üîá –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ —ç—Ö–æ TTS –ø–æ —Å–ª–æ–≤–∞–º, –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º:', normalizedRecognized, `(match ratio: ${(matchRatio * 100).toFixed(1)}%)`);
        return true;
      }
    }

    // If recognized text is very short (1-3 words) and appears in TTS, it's likely echo
    if (normalizedRecognized.length <= 20) { // Short phrases
      if (normalizedTTS.includes(normalizedRecognized)) {
        console.log('üîá –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –∫–æ—Ä–æ—Ç–∫–æ–µ —ç—Ö–æ TTS, –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º:', normalizedRecognized);
        return true;
      }
    }

    console.log('‚úÖ –¢–µ–∫—Å—Ç –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —ç—Ö–æ–º TTS');
    return false;
  };

  // Function to stop current TTS playback
  const stopCurrentTTS = useCallback(() => {
    if (currentAudioRef.current) {
      console.log('üõë –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–æ –ø—Ä–µ—Ä—ã–≤–∞—é —Ç–µ–∫—É—â—É—é –æ–∑–≤—É—á–∫—É...');

      // Multiple ways to ensure audio stops
      try {
        currentAudioRef.current.pause();
        currentAudioRef.current.currentTime = 0;
        currentAudioRef.current.volume = 0;
        currentAudioRef.current.muted = true;

        // Remove all event listeners
        currentAudioRef.current.onplay = null;
        currentAudioRef.current.onended = null;
        currentAudioRef.current.onerror = null;

        // Force garbage collection hint
        currentAudioRef.current.src = '';
        currentAudioRef.current.load();
      } catch (error) {
        console.log('‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏–∏ audio:', error);
      }

    currentAudioRef.current = null;
  }

  setIsSpeaking(false);
}, []);

  // Check if Web Speech API is available
  const isWebSpeechAvailable = useCallback(() => {
    const SpeechRecognition = window.SpeechRecognition ||
      (window as any).webkitSpeechRecognition ||
      (window as any).mozSpeechRecognition;
    return !!SpeechRecognition;
  }, []);

  // Transcribe audio using OpenAI Whisper API (fallback for browsers without Web Speech API)
  const transcribeWithOpenAI = useCallback(async (audioBlob: Blob): Promise<string | null> => {
    try {
      console.log('üé§ –û—Ç–ø—Ä–∞–≤–∫–∞ –∞—É–¥–∏–æ –Ω–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—é —á–µ—Ä–µ–∑ OpenAI Whisper...');
      setIsTranscribing(true);

      const formData = new FormData();
      formData.append('audio', audioBlob, 'recording.webm');

      const response = await fetch('http://localhost:3001/api/transcribe', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${token}`
        },
        body: formData
      });

      if (!response.ok) {
        const errorData = await response.json().catch(() => ({ error: 'Unknown error' }));
        throw new Error(errorData.error || 'Transcription failed');
      }

      const data = await response.json();
      console.log('‚úÖ –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞:', data.text);
      return data.text || null;
    } catch (error) {
      console.error('‚ùå –û—à–∏–±–∫–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏:', error);
      toast({
        title: "–û—à–∏–±–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è",
        description: "–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ä–µ—á—å. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑.",
        variant: "destructive"
      });
      return null;
    } finally {
      setIsTranscribing(false);
    }
  }, [token, toast]);

  // Start fallback recording (MediaRecorder + OpenAI Whisper)
  const startFallbackRecording = useCallback(async () => {
    try {
      console.log('üé§ –ó–∞–ø—É—Å–∫ fallback –∑–∞–ø–∏—Å–∏ (MediaRecorder)...');
      
      if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
        toast({
          title: "–ú–∏–∫—Ä–æ—Ñ–æ–Ω –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω",
          description: "–í–∞—à –±—Ä–∞—É–∑–µ—Ä –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∑–∞–ø–∏—Å—å –∞—É–¥–∏–æ.",
          variant: "destructive"
        });
        return false;
      }

      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      mediaStreamRef.current = stream;
      audioChunksRef.current = [];

      const mediaRecorder = new MediaRecorder(stream, {
        mimeType: MediaRecorder.isTypeSupported('audio/webm') ? 'audio/webm' : 'audio/mp4'
      });
      mediaRecorderRef.current = mediaRecorder;

      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data);
        }
      };

      mediaRecorder.start(100); // Collect data every 100ms
      console.log('‚úÖ Fallback –∑–∞–ø–∏—Å—å –Ω–∞—á–∞—Ç–∞');
      return true;
    } catch (error) {
      console.error('‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ fallback –∑–∞–ø–∏—Å–∏:', error);
      toast({
        title: "–û—à–∏–±–∫–∞ –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞",
        description: "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–æ—Å—Ç—É–ø –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É.",
        variant: "destructive"
      });
      return false;
    }
  }, [toast]);

  // Stop fallback recording and transcribe
  const stopFallbackRecording = useCallback(async () => {
    return new Promise<string | null>((resolve) => {
      if (!mediaRecorderRef.current) {
        resolve(null);
        return;
      }

      mediaRecorderRef.current.onstop = async () => {
        console.log('üõë Fallback –∑–∞–ø–∏—Å—å –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞, chunks:', audioChunksRef.current.length);
        
        // Stop all tracks
        if (mediaStreamRef.current) {
          mediaStreamRef.current.getTracks().forEach(track => track.stop());
          mediaStreamRef.current = null;
        }

        if (audioChunksRef.current.length === 0) {
          resolve(null);
          return;
        }

        const audioBlob = new Blob(audioChunksRef.current, { type: 'audio/webm' });
        audioChunksRef.current = [];

        // Transcribe using OpenAI
        const text = await transcribeWithOpenAI(audioBlob);
        resolve(text);
      };

      mediaRecorderRef.current.stop();
    });
  }, [transcribeWithOpenAI]);

  // Initialize Web Speech API
  const initializeSpeechRecognition = useCallback(() => {
    // Check if Web Speech API is supported (Chrome, Safari, Firefox, Edge)
    const SpeechRecognition = window.SpeechRecognition ||
      (window as any).webkitSpeechRecognition ||
      (window as any).mozSpeechRecognition; // Firefox support

    if (!SpeechRecognition) {
      console.log('‚ö†Ô∏è Web Speech API –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è, –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è OpenAI Whisper');
      setUseFallbackTranscription(true);
      return null;
    }

    console.log('üé§ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Web Speech API...');
    const recognition = new SpeechRecognition();

    // Configure recognition
    recognition.continuous = true; // Keep listening continuously
    recognition.interimResults = true; // Enable interim results to detect speech early
    recognition.lang = 'ru-RU'; // Russian language
    recognition.maxAlternatives = 1;

    // Event handlers
    recognition.onstart = () => {
      console.log('üéôÔ∏è Speech recognition started');
      console.log('üéôÔ∏è Recognition —Å–æ—Å—Ç–æ—è–Ω–∏–µ: started');
      setIsTranscribing(true);
    };

    recognition.onresult = async (event) => {
      // Don't process if mic is disabled
      if (!isMicEnabled) {
        console.log('üé§ –ú–∏–∫—Ä–æ—Ñ–æ–Ω –æ—Ç–∫–ª—é—á–µ–Ω, –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç');
        return;
      }

      const result = event.results[event.results.length - 1]; // Get the last result

      // –ü—Ä–æ–≤–µ—Ä—è—Ç—å –Ω–µ —Ç–æ–ª—å–∫–æ isSpeaking, –Ω–æ –∏ –Ω–∞–ª–∏—á–∏–µ –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –∞—É–¥–∏–æ
      if (!result.isFinal && (isSpeaking || currentAudioRef.current)) {
        const interimTranscript = result[0].transcript.trim();

        // –ë–æ–ª–µ–µ –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏—è (—Ç–æ–ª—å–∫–æ –ø—Ä–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ–º —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–∏)
        // –ò –ø—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–µ —ç—Ö–æ –æ—Ç TTS
        if (interimTranscript.length > 2 && result[0].confidence > 0.7) {
          // –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —ç—Ö–æ–º –æ—Ç TTS
          if (isEchoOfTTS(interimTranscript)) {
            console.log('üîá –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —ç—Ö–æ TTS:', interimTranscript);
            return; // Ignore echo
          }
          
          console.log('üõë –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ —Ä–µ—á—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é TTS...');
          console.log('üìù Interim transcript:', interimTranscript, 'Confidence:', result[0].confidence);
          stopCurrentTTS();
          // –û—á–∏—Å—Ç–∏—Ç—å —Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏—è, —á—Ç–æ–±—ã –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—Ç–∏—Ç—å –ª–æ–∂–Ω—ã–µ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è
          currentTTSTextRef.current = '';

          // –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ –≤—Ä–µ–º–µ–Ω–Ω–æ, —á—Ç–æ–±—ã –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—Ç–∏—Ç—å –ø–æ–≤—Ç–æ—Ä–Ω—ã–µ –ª–æ–∂–Ω—ã–µ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è
          if (speechRecognitionRef.current && isRecording) {
            try {
              speechRecognitionRef.current.stop();
              setIsRecording(false);
              console.log('üé§ –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –ø–æ—Å–ª–µ –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏—è TTS');
            } catch (e) {
              console.log('‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è:', e);
            }
          }
        }
      }

      if (result.isFinal) {
        const transcript = result[0].transcript.trim();
        console.log('üë§ –†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç:', transcript);
        console.log('üéØ –¢–µ–∫—Å—Ç –¥–ª—è –≤—ã–≤–æ–¥–∞:', transcript);

        // Check if this is echo from TTS
        if (isEchoOfTTS(transcript)) {
          console.log('üîá –§–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç —è–≤–ª—è–µ—Ç—Å—è —ç—Ö–æ–º TTS, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º');
          return;
        }

        if (transcript) {
          // Clear TTS text ref since user is actually speaking
          currentTTSTextRef.current = '';
          
          // Double-check TTS is stopped
          if (isSpeaking) {
            console.log('üé§ –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø—Ä–µ—Ä–≤–∞–ª –æ–∑–≤—É—á–∫—É, –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é TTS...');
            stopCurrentTTS();
          }

          // Save current transcript for context
          lastTranscriptRef.current = transcript;

          // Send to LLM and get response (include previous context if interrupted)
          const llmResponse = await sendToLLM(transcript);

          // Small delay to ensure previous TTS is fully stopped
          await new Promise(resolve => setTimeout(resolve, 100));

          // Speak the response (recognition continues automatically in continuous mode)
          await speakText(llmResponse);

          console.log('‚úÖ –û—Ç–≤–µ—Ç –æ–∑–≤—É—á–µ–Ω, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –ø—Ä–æ—Å–ª—É—à–∏–≤–∞–Ω–∏–µ...');

          // Ensure speech recognition continues after processing
          if (speechRecognitionRef.current && isRecording && !isSpeaking) {
            try {
              console.log('üîÑ –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—Ç–≤–µ—Ç–∞...');
              speechRecognitionRef.current.start();
            } catch (e: any) {
              if (e.name !== 'InvalidStateError') {
                console.error('‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞ –ø–æ—Å–ª–µ –æ—Ç–≤–µ—Ç–∞:', e);
              }
            }
          }
        }
      }
    };

    recognition.onerror = (event) => {
      console.error('‚ùå Speech recognition error:', event.error);
      setIsTranscribing(false);
    };

    recognition.onend = () => {
      console.log('üéôÔ∏è Speech recognition ended');
      setIsTranscribing(false);

      // In continuous mode, onend usually means an error occurred or intentional stop
      // Only restart if we're still in recording state and not speaking (to avoid conflicts)
      if (isRecording && !isSpeaking) {
        console.log('üîÑ –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫ –ø–æ—Å–ª–µ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–æ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏...');
        setTimeout(() => {
          // Double-check we still want to be recording
          if (speechRecognitionRef.current) {
            try {
              speechRecognitionRef.current.start();
              console.log('‚úÖ –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫ —É—Å–ø–µ—à–µ–Ω');
            } catch (e: any) {
              if (e.name !== 'InvalidStateError') {
                console.error('‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞:', e);
              }
            }
          }
        }, 1000); // Longer delay for error recovery
      }
    };

    speechRecognitionRef.current = recognition;
    console.log('‚úÖ Web Speech API –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω');
    return recognition;
  }, [isRecording, isMicEnabled, isSoundEnabled]);

  // Start speech recognition
  const startSpeechRecognition = useCallback(() => {
    if (!speechRecognitionRef.current) {
      console.log('‚ùå Speech recognition –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω');
      return;
    }

    console.log('üéôÔ∏è –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–ø—É—Å–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏...', {
      isRecording,
      isTranscribing,
      recognitionState: speechRecognitionRef.current ? 'exists' : 'null'
    });

    try {
      console.log('üéôÔ∏è –ó–∞–ø—É—Å–∫ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏...');
      speechRecognitionRef.current.start();
      console.log('‚úÖ start() –≤—ã–∑–≤–∞–Ω —É—Å–ø–µ—à–Ω–æ');
    } catch (error: any) {
      // Handle "already started" error gracefully
      if (error.name === 'InvalidStateError') {
        console.log('‚ÑπÔ∏è –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ —É–∂–µ –∑–∞–ø—É—â–µ–Ω–æ, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º');
        return;
      }
      console.error('‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ speech recognition:', error);
      console.error('‚ùå –î–µ—Ç–∞–ª–∏ –æ—à–∏–±–∫–∏:', {
        message: error.message,
        name: error.name,
        stack: error.stack
      });
      setIsTranscribing(false);
    }
  }, [isRecording, isTranscribing]);

  // Start/stop recording
  const handleStartStopRecording = useCallback(async () => {
    if (isRecording) {
      // Stop recording
      console.log('üõë –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–ø–∏—Å–∏...');
      setIsRecording(false);
      setIsTranscribing(false);

      // Check if using fallback (OpenAI Whisper) mode
      if (useFallbackTranscription || !isWebSpeechAvailable()) {
        // Stop fallback recording and transcribe
        const transcript = await stopFallbackRecording();
        
        if (transcript && transcript.trim()) {
          console.log('üéØ Fallback —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è:', transcript);
          
          // Stop any current TTS
          stopCurrentTTS();
          
          // Send to LLM
          try {
            const llmResponse = await sendToLLM(transcript);
            await speakText(llmResponse);
            console.log('‚úÖ –û—Ç–≤–µ—Ç –æ–∑–≤—É—á–µ–Ω');
          } catch (error) {
            console.error('‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—Ç–≤–µ—Ç–∞:', error);
          }
        }
      } else {
        // Web Speech API mode
        if (speechRecognitionRef.current) {
          try {
            speechRecognitionRef.current.stop();
          } catch (error) {
            console.log('Speech recognition already stopped');
          }
        }
      }
    } else {
      // Start recording (only if mic is enabled)
      if (!isMicEnabled) {
        toast({
          title: "–ú–∏–∫—Ä–æ—Ñ–æ–Ω –æ—Ç–∫–ª—é—á–µ–Ω",
          description: "–í–∫–ª—é—á–∏—Ç–µ –º–∏–∫—Ä–æ—Ñ–æ–Ω –¥–ª—è –Ω–∞—á–∞–ª–∞ –∑–∞–ø–∏—Å–∏",
          variant: "destructive"
        });
        return;
      }

      console.log('üé§ –ó–∞–ø—É—Å–∫ –∑–∞–ø–∏—Å–∏...');

      // Check if Web Speech API is available
      if (!isWebSpeechAvailable()) {
        console.log('üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è fallback —Ä–µ–∂–∏–º (OpenAI Whisper)');
        setUseFallbackTranscription(true);
        
        const started = await startFallbackRecording();
        if (started) {
          setIsRecording(true);
          console.log('üé§ Fallback –∑–∞–ø–∏—Å—å –Ω–∞—á–∞—Ç–∞');
        }
        return;
      }

      try {
        // Initialize Web Speech API if not already done
        if (!speechRecognitionRef.current) {
          const recognition = initializeSpeechRecognition();
          if (!recognition) {
            // Fallback to OpenAI Whisper if Web Speech API fails
            console.log('üîÑ –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ fallback —Ä–µ–∂–∏–º (OpenAI Whisper)');
            setUseFallbackTranscription(true);
            
            const started = await startFallbackRecording();
            if (started) {
              setIsRecording(true);
              console.log('üé§ Fallback –∑–∞–ø–∏—Å—å –Ω–∞—á–∞—Ç–∞');
            }
            return;
          }
        }

        setIsRecording(true);

        // Start speech recognition
        startSpeechRecognition();

        console.log('üé§ –ó–∞–ø–∏—Å—å –Ω–∞—á–∞—Ç–∞');
      } catch (error) {
        console.error('‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ –∑–∞–ø–∏—Å–∏:', error);
        
        // Try fallback on error
        console.log('üîÑ –û—à–∏–±–∫–∞ Web Speech API, –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ fallback');
        setUseFallbackTranscription(true);
        
        const started = await startFallbackRecording();
        if (started) {
          setIsRecording(true);
        }
      }
    }
  }, [isRecording, isMicEnabled, toast]);

  // Toggle microphone
  const handleToggleMic = useCallback(() => {
    if (isMicEnabled) {
      // Disable mic
      console.log('üé§ –û—Ç–∫–ª—é—á–µ–Ω–∏–µ –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞...');
      setIsMicEnabled(false);
      if (isRecording) {
        // Stop recording if it's active
        setIsRecording(false);
        setIsTranscribing(false);
        
        // Stop Web Speech API if active
        if (speechRecognitionRef.current) {
          try {
            speechRecognitionRef.current.stop();
          } catch (error) {
            console.log('Speech recognition already stopped');
          }
        }
        
        // Stop fallback recording if active
        if (mediaRecorderRef.current) {
          try {
            mediaRecorderRef.current.stop();
          } catch (error) {
            console.log('MediaRecorder already stopped');
          }
        }
        if (mediaStreamRef.current) {
          mediaStreamRef.current.getTracks().forEach(track => track.stop());
          mediaStreamRef.current = null;
        }
      }
      toast({
        title: "–ú–∏–∫—Ä–æ—Ñ–æ–Ω –æ—Ç–∫–ª—é—á–µ–Ω",
        description: "–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ –ø—Ä–∏–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ"
      });
    } else {
      // Enable mic
      console.log('üé§ –í–∫–ª—é—á–µ–Ω–∏–µ –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞...');
      setIsMicEnabled(true);
      toast({
        title: "–ú–∏–∫—Ä–æ—Ñ–æ–Ω –≤–∫–ª—é—á–µ–Ω",
        description: "–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ –∞–∫—Ç–∏–≤–Ω–æ"
      });
    }
  }, [isMicEnabled, isRecording, toast]);

  // Toggle sound
  const handleToggleSound = useCallback(() => {
    if (isSoundEnabled) {
      // Disable sound
      console.log('üîä –û—Ç–∫–ª—é—á–µ–Ω–∏–µ –∑–≤—É–∫–∞...');
      setIsSoundEnabled(false);
      toast({
        title: "–ó–≤—É–∫ –æ—Ç–∫–ª—é—á–µ–Ω",
        description: "–û—Ç–≤–µ—Ç—ã –Ω–µ –±—É–¥—É—Ç –æ–∑–≤—É—á–∏–≤–∞—Ç—å—Å—è"
      });
    } else {
      // Enable sound
      console.log('üîä –í–∫–ª—é—á–µ–Ω–∏–µ –∑–≤—É–∫–∞...');
      setIsSoundEnabled(true);
      toast({
        title: "–ó–≤—É–∫ –≤–∫–ª—é—á–µ–Ω",
        description: "–û—Ç–≤–µ—Ç—ã –±—É–¥—É—Ç –æ–∑–≤—É—á–∏–≤–∞—Ç—å—Å—è"
      });
    }
  }, [isSoundEnabled, toast]);

  // Get user profile from API
  const getUserProfile = useCallback(async () => {
    try {
      const response = await fetch('http://localhost:3001/api/profile', {
        headers: {
          'Authorization': `Bearer ${token}`
        }
      });

      if (response.ok) {
        const profile = await response.json();
        setUserProfile(profile);
        console.log('üìã –ü—Ä–æ—Ñ–∏–ª—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∑–∞–≥—Ä—É–∂–µ–Ω:', profile);
        return profile;
      }
    } catch (error) {
      console.error('‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø—Ä–æ—Ñ–∏–ª—è:', error);
    }
    return null;
  }, [token]);

  // Get course name from courseId
  const getCourseName = useCallback(() => {
    return getCourseDisplayName(courseId || "");
  }, [courseId]);

  // Send transcribed text to LLM with Julia's system prompt
  const sendToLLM = useCallback(async (userMessage: string): Promise<string> => {
    setIsGeneratingResponse(true);

    try {
      console.log('ü§ñ –û—Ç–ø—Ä–∞–≤–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏—è –≤ LLM...');

      // Get user profile if not loaded
      let profile = userProfile;
      if (!profile) {
        profile = await getUserProfile();
      }

      // Get course information
      const courseName = getCourseDisplayName(courseId || "");

      // Build context information
      const contextInfo = [];
      if (courseName) {
        contextInfo.push(`–ö—É—Ä—Å: ${courseName}`);
      }
      if (profile) {
        console.log('üìä –ü—Ä–æ—Ñ–∏–ª—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è LLM:', profile);
        if (profile.learning_style) {
          contextInfo.push(`–°—Ç–∏–ª—å –æ–±—É—á–µ–Ω–∏—è: ${profile.learning_style}`);
        }
        if (profile.difficulty_level) {
          contextInfo.push(`–£—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏: ${profile.difficulty_level}`);
        }
        if (profile.interests) {
          // Safely handle interests - could be string, array, or object
          let interestsStr = '';
          if (typeof profile.interests === 'string') {
            interestsStr = profile.interests;
          } else if (Array.isArray(profile.interests)) {
            interestsStr = profile.interests.join(', ');
          } else if (profile.interests) {
            interestsStr = JSON.stringify(profile.interests);
          }
          if (interestsStr) {
            contextInfo.push(`–ò–Ω—Ç–µ—Ä–µ—Å—ã: ${interestsStr}`);
          }
        }
      }

      const contextString = contextInfo.length > 0 ? `\n–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ:\n${contextInfo.join('\n')}` : '';

      // NOTE:
      // –†–∞–Ω—å—à–µ –º—ã —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–ª–∏ –∑–¥–µ—Å—å –ø–æ–ª–Ω—ã–π –ø—Ä–æ–º—Ç —É—á–∏—Ç–µ–ª—è –Æ–ª–∏–∏ (teacherJuliaPrompt)
      // –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–ª–∏ –µ–≥–æ –∫–∞–∫ content. –¢–µ–ø–µ—Ä—å –≠–¢–û –î–ï–õ–ê–ï–¢ –°–ï–†–í–ï–†:
      //  - —Å–µ—Ä–≤–µ—Ä –∑–Ω–∞–µ—Ç –∫—É—Ä—Å, –ø—Ä–æ—Ñ–∏–ª—å, –¥–æ–º–∞—à–∫–∏
      //  - —Å–∞–º –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º—Ç (generateVoiceChatPrompt)
      // –ü–æ—ç—Ç–æ–º—É —Å —Ñ—Ä–æ–Ω—Ç–∞ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –¢–û–õ–¨–ö–û —á–∏—Å—Ç—É—é —Ä–µ–ø–ª–∏–∫—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.

      // Prepare message with context if TTS was interrupted
      let messageContent = userMessage;
      if (isSpeaking && lastTranscriptRef.current && lastTranscriptRef.current !== userMessage) {
        // Include previous context when TTS was interrupted
        messageContent = `–ü—Ä–µ–¥—ã–¥—É—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç: "${lastTranscriptRef.current}". –ù–æ–≤—ã–π –≤–æ–ø—Ä–æ—Å: "${userMessage}"`;
        console.log('üìù –ü–µ—Ä–µ–¥–∞—é –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–µ—Ä–≤–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞:', messageContent);
      }

      // Send raw user message to server API
      console.log('üöÄ –û—Ç–ø—Ä–∞–≤–∫–∞ –≤ VoiceChat:', {
        url: `http://localhost:3001/api/chat/${courseId}/message`,
        content: messageContent,
        messageType: 'voice',
        token: token ? 'present' : 'missing'
      });

      const response = await fetch(`http://localhost:3001/api/chat/${courseId}/message`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${token}`
        },
        body: JSON.stringify({
          // –í content –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –µ—Å–ª–∏ –Ω—É–∂–Ω–æ.
          // –°–µ—Ä–≤–µ—Ä –ø–æ—Å—Ç—Ä–æ–∏—Ç —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º—Ç —É—á–∏—Ç–µ–ª—è –Æ–ª–∏–∏ —Å–∞–º.
          content: messageContent,
          messageType: 'voice',
          interrupted: isSpeaking // Flag to indicate if this was an interruption
        })
      });

      if (!response.ok) {
        let errorData = { error: 'Unknown error' };
        try {
          const text = await response.text();
          if (text) {
            errorData = JSON.parse(text);
          }
        } catch (parseError) {
          console.error('‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ—Ç–≤–µ—Ç–∞:', parseError);
        }
        console.error('‚ùå –û—Ç–≤–µ—Ç —Å–µ—Ä–≤–µ—Ä–∞ —Å –æ—à–∏–±–∫–æ–π:', response.status, errorData);
        throw new Error(errorData.error || errorData.details || 'Failed to get LLM response');
      }

      // Parse response safely
      let data;
      try {
        const text = await response.text();
        console.log('üì• –°—ã—Ä–æ–π –æ—Ç–≤–µ—Ç —Å–µ—Ä–≤–µ—Ä–∞:', text.substring(0, 200));
        data = JSON.parse(text);
      } catch (parseError) {
        console.error('‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON:', parseError);
        throw new Error('Invalid JSON response from server');
      }

      console.log('‚úÖ LLM –æ—Ç–≤–µ—Ç–∏–ª:', data.message);

      return data.message || '–ò–∑–≤–∏–Ω–∏, —è –Ω–µ —Å–º–æ–≥–ª–∞ —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç. –ü–æ–ø—Ä–æ–±—É–π –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å.';

    } catch (error) {
      console.error('‚ùå –û—à–∏–±–∫–∞ LLM:', error);
      console.error('‚ùå –î–µ—Ç–∞–ª–∏ –æ—à–∏–±–∫–∏:', {
        message: error.message,
        stack: error.stack,
        name: error.name
      });
      return '–ò–∑–≤–∏–Ω–∏, —É –º–µ–Ω—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –Ω–µ–ø–æ–ª–∞–¥–∫–∏. –ü–æ–ø—Ä–æ–±—É–π –µ—â–µ —Ä–∞–∑ —á–µ—Ä–µ–∑ –º–∏–Ω—É—Ç—É.';
    } finally {
      setIsGeneratingResponse(false);
    }
  }, [token, userProfile, courseId, getUserProfile]);

  // Convert text to speech using OpenAI TTS
  const speakText = useCallback(async (text: string) => {
    // Don't speak if sound is disabled
    if (!isSoundEnabled) {
      console.log('üîá –ó–≤—É–∫ –æ—Ç–∫–ª—é—á–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –æ–∑–≤—É—á–∫—É');
      return;
    }

    // Store the TTS text for echo detection
    currentTTSTextRef.current = text;
    
    setIsSpeaking(true);

    // –ê–∫—Ç–∏–≤–∏—Ä–æ–≤–∞—Ç—å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ –¥–ª—è –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏—è TTS (–µ—Å–ª–∏ –º–∏–∫—Ä–æ—Ñ–æ–Ω –≤–∫–ª—é—á–µ–Ω)
    // –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ —É–∂–µ –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback —Ä–µ–∂–∏–º
    if (isMicEnabled && !isRecording && !useFallbackTranscription && isWebSpeechAvailable()) {
      console.log('üé§ –í–∫–ª—é—á–∞—é —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ –¥–ª—è –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏—è TTS');
      try {
        if (!speechRecognitionRef.current) {
          const recognition = initializeSpeechRecognition();
          if (recognition) {
            speechRecognitionRef.current = recognition;
          }
        }
        if (speechRecognitionRef.current) {
          // –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞, —á—Ç–æ–±—ã TTS —É—Å–ø–µ–ª –Ω–∞—á–∞—Ç—å –∏–≥—Ä–∞—Ç—å –∏ –Ω–µ –±—ã–ª–æ –ª–æ–∂–Ω—ã—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π
          setTimeout(() => {
            // –ë–µ–∑–æ–ø–∞—Å–Ω–æ –ø—ã—Ç–∞–µ–º—Å—è –∑–∞–ø—É—Å—Ç–∏—Ç—å, –∏–≥–Ω–æ—Ä–∏—Ä—É—è –æ—à–∏–±–∫—É –µ—Å–ª–∏ —É–∂–µ –∑–∞–ø—É—â–µ–Ω–æ
            try {
              speechRecognitionRef.current?.start();
              setIsRecording(true);
              console.log('‚úÖ –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ –∑–∞–ø—É—â–µ–Ω–æ –¥–ª—è TTS –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏—è');
            } catch (startError: any) {
              if (startError.name === 'InvalidStateError') {
                console.log('‚ÑπÔ∏è –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ —É–∂–µ –∑–∞–ø—É—â–µ–Ω–æ');
              } else {
                console.log('‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è:', startError);
              }
            }
          }, 500); // 500ms delay
        }
      } catch (error) {
        console.error('‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø—É—Å—Ç–∏—Ç—å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏:', error);
      }
    }

    try {
      console.log('üîä –û—Ç–ø—Ä–∞–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –≤ OpenAI TTS...');

      const response = await fetch('http://localhost:3001/api/tts', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${token}`
        },
        body: JSON.stringify({
          text: text,
          voice: 'nova' // High-quality educational voice (HD model)
        })
      });

      if (!response.ok) {
        const errorData = await response.json().catch(() => ({ error: 'Unknown error' }));
        throw new Error(errorData.error || errorData.details || 'Failed to generate speech');
      }

      // Get audio blob
      const audioBlob = await response.blob();
      console.log('‚úÖ –ü–æ–ª—É—á–µ–Ω –∞—É–¥–∏–æ —Ñ–∞–π–ª, —Ä–∞–∑–º–µ—Ä:', audioBlob.size);

      // –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø—Ä–µ–¥—ã–¥—É—â–µ–µ –∞—É–¥–∏–æ –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º –Ω–æ–≤–æ–≥–æ
      // –≠—Ç–æ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –Ω–∞–ª–æ–∂–µ–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö TTS –ø–æ—Ç–æ–∫–æ–≤
      if (currentAudioRef.current) {
        stopCurrentTTS();
      }

      // –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ —á—Ç–æ–±—ã —É–±–µ–¥–∏—Ç—å—Å—è —á—Ç–æ –ø—Ä–µ–¥—ã–¥—É—â–µ–µ –∞—É–¥–∏–æ –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ
      await new Promise(resolve => setTimeout(resolve, 100));

      // Create audio element and play
      const audioUrl = URL.createObjectURL(audioBlob);
      const audio = new Audio(audioUrl);
      currentAudioRef.current = audio;

      // Event handlers
      audio.onplay = () => {
        console.log('üîä –û–∑–≤—É—á–∫–∞ –Ω–∞—á–∞—Ç–∞');
      };

      audio.onended = () => {
        console.log('‚úÖ –û–∑–≤—É—á–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞');
        URL.revokeObjectURL(audioUrl);
        currentAudioRef.current = null;
        currentTTSTextRef.current = ''; // Clear TTS text for echo detection
        setIsSpeaking(false);

        // –ù–ï –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ - –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –ø—Ä–æ—Å–ª—É—à–∏–≤–∞–Ω–∏–µ –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ –≤–æ–ø—Ä–æ—Å–∞
        console.log('üé§ –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –ø—Ä–æ—Å–ª—É—à–∏–≤–∞–Ω–∏–µ –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ –≤–æ–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è');
      };

      audio.onerror = (event) => {
        console.error('‚ùå –û—à–∏–±–∫–∞ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –∞—É–¥–∏–æ:', event);
        URL.revokeObjectURL(audioUrl);
        currentAudioRef.current = null;
        setIsSpeaking(false);

        // –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
        if (speechRecognitionRef.current && isRecording) {
          try {
            speechRecognitionRef.current.stop();
            setIsRecording(false);
          } catch (e) {
            console.log('‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –ø—Ä–∏ –æ—à–∏–±–∫–µ TTS:', e);
          }
        }
        toast({
          title: "–û—à–∏–±–∫–∞ –æ–∑–≤—É—á–∫–∏",
          description: "–ù–µ —É–¥–∞–ª–æ—Å—å –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ—Å—Ç–∏ –∞—É–¥–∏–æ",
          variant: "destructive"
        });
      };

      // Small delay to ensure clean start
      await new Promise(resolve => setTimeout(resolve, 50));

      // Start playing
      await audio.play();

    } catch (error) {
      console.error('‚ùå –û—à–∏–±–∫–∞ TTS:', error);
      setIsSpeaking(false);

      // –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
      if (speechRecognitionRef.current && isRecording) {
        try {
          speechRecognitionRef.current.stop();
          setIsRecording(false);
        } catch (e) {
          console.log('‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –ø—Ä–∏ –æ—à–∏–±–∫–µ TTS:', e);
        }
      }

      toast({
        title: "–û—à–∏–±–∫–∞ –æ–∑–≤—É—á–∫–∏",
        description: `–ù–µ —É–¥–∞–ª–æ—Å—å –æ–∑–≤—É—á–∏—Ç—å —Ç–µ–∫—Å—Ç: ${error.message}`,
        variant: "destructive"
      });
    }
  }, [token, toast, isSoundEnabled, stopCurrentTTS]);

  // Load user profile on component mount
  useEffect(() => {
    if (token) {
      getUserProfile();
    }
  }, [token, getUserProfile]);

  // Cleanup on unmount
  useEffect(() => {
    return () => {
      console.log('üßπ –û—á–∏—Å—Ç–∫–∞ –ø—Ä–∏ —Ä–∞–∑–º–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏');
      // Stop Web Speech API
      if (speechRecognitionRef.current) {
        try {
          speechRecognitionRef.current.stop();
        } catch (error) {
          // Already stopped
        }
      }
      // Stop fallback MediaRecorder
      if (mediaRecorderRef.current) {
        try {
          mediaRecorderRef.current.stop();
        } catch (error) {
          // Already stopped
        }
      }
      // Stop media stream
      if (mediaStreamRef.current) {
        mediaStreamRef.current.getTracks().forEach(track => track.stop());
      }
      // Stop current TTS
      stopCurrentTTS();
      // Stop speech synthesis (fallback)
      if ('speechSynthesis' in window) {
        window.speechSynthesis.cancel();
      }
    };
  }, [stopCurrentTTS]);

  return (
    <div className="min-h-screen bg-background">
      <Navigation />


      <main className="container mx-auto px-4 pt-24 pb-16">
        <div className="max-w-2xl mx-auto">
          <div className="text-center mb-8 animate-fade-in">
            <h1 className="text-3xl md:text-4xl font-bold mb-4 bg-gradient-to-r from-primary to-emerald-600 bg-clip-text text-transparent">
              –ì–æ–ª–æ—Å–æ–≤–æ–µ –æ–±—â–µ–Ω–∏–µ
            </h1>
            <p className="text-muted-foreground">
              {getCourseDisplayName(courseId || "")}
            </p>
          </div>

          <Card className="shadow-2xl animate-fade-in">
            <CardContent className="p-8 md:p-12">
              <div className="text-center space-y-6">
                {/* Status Indicator */}
                <div className="relative inline-block">
                  <div className={`w-32 h-32 md:w-40 md:h-40 rounded-full ${
                    isRecording ? 'bg-green-500/20' : 'bg-muted'
                  } flex items-center justify-center transition-all duration-300`}>
                    <Mic className={`w-16 h-16 md:w-20 md:h-20 ${
                      isRecording ? 'text-green-500' : 'text-muted-foreground'
                    }`} />
                  </div>
                  {isRecording && (
                    <div className="absolute inset-0 rounded-full animate-ping bg-green-500/20"></div>
                  )}
                </div>

                {/* Status Text */}
                <div>
                  <h2 className="text-2xl font-bold mb-2">
                    {isRecording ? "–ò–¥–µ—Ç –∑–∞–ø–∏—Å—å –∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ" : "–ì–æ—Ç–æ–≤ –∫ –∑–∞–ø–∏—Å–∏"}
                  </h2>
                  <p className="text-muted-foreground mb-3">
                    {isRecording
                      ? "–ì–æ–≤–æ—Ä–∏—Ç–µ —Å–≤–æ–±–æ–¥–Ω–æ - —Ç–µ–∫—Å—Ç –≤—ã–≤–æ–¥–∏—Ç—Å—è –≤ –∫–æ–Ω—Å–æ–ª—å"
                      : "–ù–∞–∂–º–∏—Ç–µ –∫–Ω–æ–ø–∫—É –¥–ª—è –Ω–∞—á–∞–ª–∞ –∑–∞–ø–∏—Å–∏ –∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏"
                    }
                  </p>

                  {/* Recording Status */}
                  <div className="flex flex-wrap justify-center gap-2">
                    {isRecording && (
                      <Badge variant="secondary" className="bg-green-100 text-green-700 border-green-200 animate-pulse">
                        üé§ {useFallbackTranscription ? '–ó–∞–ø–∏—Å—å (OpenAI)...' : '–ó–∞–ø–∏—Å—å –∞–∫—Ç–∏–≤–Ω–∞...'}
                      </Badge>
                    )}
                    {isTranscribing && (
                      <Badge variant="outline" className="bg-blue-50 text-blue-700 border-blue-200">
                        <Loader2 className="w-3 h-3 mr-1 animate-spin" />
                        –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ...
                      </Badge>
                    )}
                    {isGeneratingResponse && (
                      <Badge variant="outline" className="bg-purple-50 text-purple-700 border-purple-200">
                        <Loader2 className="w-3 h-3 mr-1 animate-spin" />
                        ü§ñ –î—É–º–∞—é...
                      </Badge>
                    )}
                    {isSpeaking && (
                      <Badge variant="outline" className="bg-orange-50 text-orange-700 border-orange-200 animate-pulse">
                        üîä –ì–æ–≤–æ—Ä—é...
                      </Badge>
                    )}
                    {!isRecording && (
                      <Badge variant="outline" className="bg-gray-50 text-gray-700 border-gray-200">
                        üîá –û–∂–∏–¥–∞–Ω–∏–µ –Ω–∞—á–∞–ª–∞
                      </Badge>
                    )}
                  </div>
                </div>

                {/* Control Buttons */}
                <div className="flex flex-wrap justify-center gap-4">
                  <Button
                    variant="outline"
                    size="lg"
                    className={`w-16 h-16 rounded-full ${isMicEnabled ? 'bg-green-50 border-green-200 hover:bg-green-100' : 'bg-red-50 border-red-200 hover:bg-red-100'}`}
                    onClick={handleToggleMic}
                    title={isMicEnabled ? "–û—Ç–∫–ª—é—á–∏—Ç—å –º–∏–∫—Ä–æ—Ñ–æ–Ω" : "–í–∫–ª—é—á–∏—Ç—å –º–∏–∫—Ä–æ—Ñ–æ–Ω"}
                  >
                    {isMicEnabled ? (
                      <Mic className="w-6 h-6 text-green-600" />
                    ) : (
                      <MicOff className="w-6 h-6 text-red-600" />
                    )}
                  </Button>
                  <Button
                    variant="outline"
                    size="lg"
                    className={`w-16 h-16 rounded-full ${isSoundEnabled ? 'bg-blue-50 border-blue-200 hover:bg-blue-100' : 'bg-gray-50 border-gray-200 hover:bg-gray-100'}`}
                    onClick={handleToggleSound}
                    title={isSoundEnabled ? "–û—Ç–∫–ª—é—á–∏—Ç—å –∑–≤—É–∫" : "–í–∫–ª—é—á–∏—Ç—å –∑–≤—É–∫"}
                  >
                    {isSoundEnabled ? (
                      <Volume2 className="w-6 h-6 text-blue-600" />
                    ) : (
                      <VolumeX className="w-6 h-6 text-gray-600" />
                    )}
                  </Button>
                </div>

                {/* Main Action Button */}
                <Button
                  size="lg"
                  className={`w-full max-w-xs h-14 text-lg ${isRecording ? 'bg-red-500 hover:bg-red-600' : ''}`}
                  onClick={handleStartStopRecording}
                >
                  {isRecording ? (
                    <>
                      <PhoneOff className="w-5 h-5 mr-2" />
                      –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∑–∞–ø–∏—Å—å
                    </>
                  ) : (
                    <>
                      <Phone className="w-5 h-5 mr-2" />
                      –ù–∞—á–∞—Ç—å —É—Ä–æ–∫
                    </>
                  )}
                </Button>
              </div>
            </CardContent>
          </Card>

        </div>
      </main>
    </div>
  );
};

export default VoiceChat;
